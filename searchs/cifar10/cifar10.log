06/28 09:22:15 PM | 
06/28 09:22:15 PM | Parameters:
06/28 09:22:15 PM | ALPHA_LR=0.0003
06/28 09:22:15 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:22:15 PM | BATCH_SIZE=64
06/28 09:22:15 PM | DATA_PATH=./data/
06/28 09:22:15 PM | DATASET=cifar10
06/28 09:22:15 PM | EPOCHS=50
06/28 09:22:15 PM | GPUS=[0]
06/28 09:22:15 PM | INIT_CHANNELS=16
06/28 09:22:15 PM | LAYERS=8
06/28 09:22:15 PM | NAME=cifar10
06/28 09:22:15 PM | PATH=searchs/cifar10
06/28 09:22:15 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:22:15 PM | PRINT_FREQ=50
06/28 09:22:15 PM | SEED=2
06/28 09:22:15 PM | W_GRAD_CLIP=5.0
06/28 09:22:15 PM | W_LR=0.025
06/28 09:22:15 PM | W_LR_MIN=0.001
06/28 09:22:15 PM | W_MOMENTUM=0.9
06/28 09:22:15 PM | W_WEIGHT_DECAY=0.0003
06/28 09:22:15 PM | WORKERS=4
06/28 09:22:15 PM | 
06/28 09:22:15 PM | Logger is set - training start
06/28 09:28:51 PM | 
06/28 09:28:51 PM | Parameters:
06/28 09:28:51 PM | ALPHA_LR=0.0003
06/28 09:28:51 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:28:51 PM | BATCH_SIZE=64
06/28 09:28:51 PM | DATA_PATH=./data/
06/28 09:28:51 PM | DATASET=cifar10
06/28 09:28:51 PM | EPOCHS=50
06/28 09:28:51 PM | GPUS=[0]
06/28 09:28:51 PM | INIT_CHANNELS=16
06/28 09:28:51 PM | LAYERS=8
06/28 09:28:51 PM | NAME=cifar10
06/28 09:28:51 PM | PATH=searchs/cifar10
06/28 09:28:51 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:28:51 PM | PRINT_FREQ=50
06/28 09:28:51 PM | SEED=2
06/28 09:28:51 PM | W_GRAD_CLIP=5.0
06/28 09:28:51 PM | W_LR=0.025
06/28 09:28:51 PM | W_LR_MIN=0.001
06/28 09:28:51 PM | W_MOMENTUM=0.9
06/28 09:28:51 PM | W_WEIGHT_DECAY=0.0003
06/28 09:28:51 PM | WORKERS=4
06/28 09:28:51 PM | 
06/28 09:28:51 PM | Logger is set - training start
06/28 09:30:37 PM | 
06/28 09:30:37 PM | Parameters:
06/28 09:30:37 PM | ALPHA_LR=0.0003
06/28 09:30:37 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:30:37 PM | BATCH_SIZE=64
06/28 09:30:37 PM | DATA_PATH=./data/
06/28 09:30:37 PM | DATASET=cifar10
06/28 09:30:37 PM | EPOCHS=50
06/28 09:30:37 PM | GPUS=[0]
06/28 09:30:37 PM | INIT_CHANNELS=16
06/28 09:30:37 PM | LAYERS=8
06/28 09:30:37 PM | NAME=cifar10
06/28 09:30:37 PM | PATH=searchs/cifar10
06/28 09:30:37 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:30:37 PM | PRINT_FREQ=50
06/28 09:30:37 PM | SEED=2
06/28 09:30:37 PM | W_GRAD_CLIP=5.0
06/28 09:30:37 PM | W_LR=0.025
06/28 09:30:37 PM | W_LR_MIN=0.001
06/28 09:30:37 PM | W_MOMENTUM=0.9
06/28 09:30:37 PM | W_WEIGHT_DECAY=0.0003
06/28 09:30:37 PM | WORKERS=4
06/28 09:30:37 PM | 
06/28 09:30:37 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:31:11 PM | 
06/28 09:31:11 PM | Parameters:
06/28 09:31:11 PM | ALPHA_LR=0.0003
06/28 09:31:11 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:31:11 PM | BATCH_SIZE=64
06/28 09:31:11 PM | DATA_PATH=./data/
06/28 09:31:11 PM | DATASET=cifar10
06/28 09:31:11 PM | EPOCHS=50
06/28 09:31:11 PM | GPUS=[0]
06/28 09:31:11 PM | INIT_CHANNELS=16
06/28 09:31:11 PM | LAYERS=8
06/28 09:31:11 PM | NAME=cifar10
06/28 09:31:11 PM | PATH=searchs/cifar10
06/28 09:31:11 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:31:11 PM | PRINT_FREQ=50
06/28 09:31:11 PM | SEED=2
06/28 09:31:11 PM | W_GRAD_CLIP=5.0
06/28 09:31:11 PM | W_LR=0.025
06/28 09:31:11 PM | W_LR_MIN=0.001
06/28 09:31:11 PM | W_MOMENTUM=0.9
06/28 09:31:11 PM | W_WEIGHT_DECAY=0.0003
06/28 09:31:11 PM | WORKERS=4
06/28 09:31:11 PM | 
06/28 09:31:11 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:31:44 PM | 
06/28 09:31:44 PM | Parameters:
06/28 09:31:44 PM | ALPHA_LR=0.0003
06/28 09:31:44 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:31:44 PM | BATCH_SIZE=64
06/28 09:31:44 PM | DATA_PATH=./data/
06/28 09:31:44 PM | DATASET=cifar10
06/28 09:31:44 PM | EPOCHS=50
06/28 09:31:44 PM | GPUS=[0]
06/28 09:31:44 PM | INIT_CHANNELS=16
06/28 09:31:44 PM | LAYERS=8
06/28 09:31:44 PM | NAME=cifar10
06/28 09:31:44 PM | PATH=searchs/cifar10
06/28 09:31:44 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:31:44 PM | PRINT_FREQ=50
06/28 09:31:44 PM | SEED=2
06/28 09:31:44 PM | W_GRAD_CLIP=5.0
06/28 09:31:44 PM | W_LR=0.025
06/28 09:31:44 PM | W_LR_MIN=0.001
06/28 09:31:44 PM | W_MOMENTUM=0.9
06/28 09:31:44 PM | W_WEIGHT_DECAY=0.0003
06/28 09:31:44 PM | WORKERS=4
06/28 09:31:44 PM | 
06/28 09:31:44 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:37:36 PM | 
06/28 09:37:36 PM | Parameters:
06/28 09:37:36 PM | ALPHA_LR=0.0003
06/28 09:37:36 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:37:36 PM | BATCH_SIZE=64
06/28 09:37:36 PM | DATA_PATH=./data/
06/28 09:37:36 PM | DATASET=cifar10
06/28 09:37:36 PM | EPOCHS=50
06/28 09:37:36 PM | GPUS=[0]
06/28 09:37:36 PM | INIT_CHANNELS=16
06/28 09:37:36 PM | LAYERS=8
06/28 09:37:36 PM | NAME=cifar10
06/28 09:37:36 PM | PATH=searchs/cifar10
06/28 09:37:36 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:37:36 PM | PRINT_FREQ=50
06/28 09:37:36 PM | SEED=2
06/28 09:37:36 PM | W_GRAD_CLIP=5.0
06/28 09:37:36 PM | W_LR=0.025
06/28 09:37:36 PM | W_LR_MIN=0.001
06/28 09:37:36 PM | W_MOMENTUM=0.9
06/28 09:37:36 PM | W_WEIGHT_DECAY=0.0003
06/28 09:37:36 PM | WORKERS=4
06/28 09:37:36 PM | 
06/28 09:37:36 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:38:10 PM | 
06/28 09:38:10 PM | Parameters:
06/28 09:38:10 PM | ALPHA_LR=0.0003
06/28 09:38:10 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:38:10 PM | BATCH_SIZE=64
06/28 09:38:10 PM | DATA_PATH=./data/
06/28 09:38:10 PM | DATASET=cifar10
06/28 09:38:10 PM | EPOCHS=50
06/28 09:38:10 PM | GPUS=[0]
06/28 09:38:10 PM | INIT_CHANNELS=16
06/28 09:38:10 PM | LAYERS=8
06/28 09:38:10 PM | NAME=cifar10
06/28 09:38:10 PM | PATH=searchs/cifar10
06/28 09:38:10 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:38:10 PM | PRINT_FREQ=50
06/28 09:38:10 PM | SEED=2
06/28 09:38:10 PM | W_GRAD_CLIP=5.0
06/28 09:38:10 PM | W_LR=0.025
06/28 09:38:10 PM | W_LR_MIN=0.001
06/28 09:38:10 PM | W_MOMENTUM=0.9
06/28 09:38:10 PM | W_WEIGHT_DECAY=0.0003
06/28 09:38:10 PM | WORKERS=4
06/28 09:38:10 PM | 
06/28 09:38:10 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:40:03 PM | 
06/28 09:40:03 PM | Parameters:
06/28 09:40:03 PM | ALPHA_LR=0.0003
06/28 09:40:03 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:40:03 PM | BATCH_SIZE=64
06/28 09:40:03 PM | DATA_PATH=./data/
06/28 09:40:03 PM | DATASET=cifar10
06/28 09:40:03 PM | EPOCHS=50
06/28 09:40:03 PM | GPUS=[0]
06/28 09:40:03 PM | INIT_CHANNELS=16
06/28 09:40:03 PM | LAYERS=8
06/28 09:40:03 PM | NAME=cifar10
06/28 09:40:03 PM | PATH=searchs/cifar10
06/28 09:40:03 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:40:03 PM | PRINT_FREQ=50
06/28 09:40:03 PM | SEED=2
06/28 09:40:03 PM | W_GRAD_CLIP=5.0
06/28 09:40:03 PM | W_LR=0.025
06/28 09:40:03 PM | W_LR_MIN=0.001
06/28 09:40:03 PM | W_MOMENTUM=0.9
06/28 09:40:03 PM | W_WEIGHT_DECAY=0.0003
06/28 09:40:03 PM | WORKERS=4
06/28 09:40:03 PM | 
06/28 09:40:03 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:41:07 PM | 
06/28 09:41:07 PM | Parameters:
06/28 09:41:07 PM | ALPHA_LR=0.0003
06/28 09:41:07 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:41:07 PM | BATCH_SIZE=64
06/28 09:41:07 PM | DATA_PATH=./data/
06/28 09:41:07 PM | DATASET=cifar10
06/28 09:41:07 PM | EPOCHS=50
06/28 09:41:07 PM | GPUS=[0]
06/28 09:41:07 PM | INIT_CHANNELS=16
06/28 09:41:07 PM | LAYERS=8
06/28 09:41:07 PM | NAME=cifar10
06/28 09:41:07 PM | PATH=searchs/cifar10
06/28 09:41:07 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:41:07 PM | PRINT_FREQ=50
06/28 09:41:07 PM | SEED=2
06/28 09:41:07 PM | W_GRAD_CLIP=5.0
06/28 09:41:07 PM | W_LR=0.025
06/28 09:41:07 PM | W_LR_MIN=0.001
06/28 09:41:07 PM | W_MOMENTUM=0.9
06/28 09:41:07 PM | W_WEIGHT_DECAY=0.0003
06/28 09:41:07 PM | WORKERS=4
06/28 09:41:07 PM | 
06/28 09:41:07 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:50:00 PM | 
06/28 09:50:00 PM | Parameters:
06/28 09:50:00 PM | ALPHA_LR=0.0003
06/28 09:50:00 PM | ALPHA_WEIGHT_DECAY=0.001
06/28 09:50:00 PM | BATCH_SIZE=32
06/28 09:50:00 PM | DATA_PATH=./data/
06/28 09:50:00 PM | DATASET=cifar10
06/28 09:50:00 PM | EPOCHS=50
06/28 09:50:00 PM | GPUS=[0]
06/28 09:50:00 PM | INIT_CHANNELS=16
06/28 09:50:00 PM | LAYERS=8
06/28 09:50:00 PM | NAME=cifar10
06/28 09:50:00 PM | PATH=searchs/cifar10
06/28 09:50:00 PM | PLOT_PATH=searchs/cifar10/plots
06/28 09:50:00 PM | PRINT_FREQ=50
06/28 09:50:00 PM | SEED=2
06/28 09:50:00 PM | W_GRAD_CLIP=5.0
06/28 09:50:00 PM | W_LR=0.025
06/28 09:50:00 PM | W_LR_MIN=0.001
06/28 09:50:00 PM | W_MOMENTUM=0.9
06/28 09:50:00 PM | W_WEIGHT_DECAY=0.0003
06/28 09:50:00 PM | WORKERS=4
06/28 09:50:00 PM | 
06/28 09:50:00 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 09:50:10 PM | Train: [ 1/50] Step 000/781 Loss 2.258 Prec@(1,5) (25.0%, 75.0%)
06/28 09:53:50 PM | Train: [ 1/50] Step 050/781 Loss 2.148 Prec@(1,5) (22.8%, 77.6%)
06/28 09:57:30 PM | Train: [ 1/50] Step 100/781 Loss 2.041 Prec@(1,5) (25.8%, 80.4%)
06/28 10:01:10 PM | Train: [ 1/50] Step 150/781 Loss 1.968 Prec@(1,5) (27.4%, 82.4%)
06/28 10:04:50 PM | Train: [ 1/50] Step 200/781 Loss 1.907 Prec@(1,5) (30.1%, 83.9%)
06/28 10:08:30 PM | Train: [ 1/50] Step 250/781 Loss 1.861 Prec@(1,5) (31.5%, 85.0%)
06/28 10:12:10 PM | Train: [ 1/50] Step 300/781 Loss 1.822 Prec@(1,5) (32.9%, 85.8%)
06/28 10:15:50 PM | Train: [ 1/50] Step 350/781 Loss 1.792 Prec@(1,5) (34.1%, 86.3%)
06/28 10:19:30 PM | Train: [ 1/50] Step 400/781 Loss 1.760 Prec@(1,5) (35.2%, 86.9%)
06/28 10:23:10 PM | Train: [ 1/50] Step 450/781 Loss 1.738 Prec@(1,5) (36.0%, 87.4%)
06/28 10:26:50 PM | Train: [ 1/50] Step 500/781 Loss 1.711 Prec@(1,5) (37.2%, 87.8%)
06/28 10:30:30 PM | Train: [ 1/50] Step 550/781 Loss 1.687 Prec@(1,5) (38.2%, 88.2%)
06/28 10:34:10 PM | Train: [ 1/50] Step 600/781 Loss 1.665 Prec@(1,5) (39.1%, 88.5%)
06/28 10:37:49 PM | Train: [ 1/50] Step 650/781 Loss 1.639 Prec@(1,5) (40.1%, 88.9%)
06/28 10:41:29 PM | Train: [ 1/50] Step 700/781 Loss 1.621 Prec@(1,5) (40.9%, 89.2%)
06/28 10:45:09 PM | Train: [ 1/50] Step 750/781 Loss 1.602 Prec@(1,5) (41.6%, 89.5%)
06/28 10:47:25 PM | Train: [ 1/50] Step 781/781 Loss 1.589 Prec@(1,5) (42.2%, 89.7%)
06/28 10:47:25 PM | Train: [ 1/50] Final Prec@1 42.2200%
06/28 10:47:25 PM | Valid: [ 1/50] Step 000/781 Loss 1.273 Prec@(1,5) (50.0%, 93.8%)
06/28 10:47:36 PM | Valid: [ 1/50] Step 050/781 Loss 1.343 Prec@(1,5) (52.2%, 94.2%)
06/28 10:47:47 PM | Valid: [ 1/50] Step 100/781 Loss 1.340 Prec@(1,5) (52.0%, 94.3%)
06/28 10:47:57 PM | Valid: [ 1/50] Step 150/781 Loss 1.340 Prec@(1,5) (51.9%, 94.4%)
06/28 10:48:08 PM | Valid: [ 1/50] Step 200/781 Loss 1.345 Prec@(1,5) (52.1%, 94.0%)
06/28 10:48:19 PM | Valid: [ 1/50] Step 250/781 Loss 1.330 Prec@(1,5) (52.7%, 94.2%)
06/28 10:48:30 PM | Valid: [ 1/50] Step 300/781 Loss 1.327 Prec@(1,5) (53.2%, 94.2%)
06/28 10:48:41 PM | Valid: [ 1/50] Step 350/781 Loss 1.337 Prec@(1,5) (53.0%, 94.2%)
06/28 10:48:53 PM | Valid: [ 1/50] Step 400/781 Loss 1.336 Prec@(1,5) (53.0%, 94.2%)
06/28 10:49:04 PM | Valid: [ 1/50] Step 450/781 Loss 1.338 Prec@(1,5) (53.0%, 94.1%)
06/28 10:49:14 PM | Valid: [ 1/50] Step 500/781 Loss 1.339 Prec@(1,5) (52.9%, 94.1%)
06/28 10:49:25 PM | Valid: [ 1/50] Step 550/781 Loss 1.340 Prec@(1,5) (53.0%, 94.0%)
06/28 10:49:36 PM | Valid: [ 1/50] Step 600/781 Loss 1.343 Prec@(1,5) (52.9%, 94.0%)
06/28 10:49:48 PM | Valid: [ 1/50] Step 650/781 Loss 1.342 Prec@(1,5) (52.9%, 94.0%)
06/28 10:49:59 PM | Valid: [ 1/50] Step 700/781 Loss 1.340 Prec@(1,5) (53.0%, 94.0%)
06/28 10:50:09 PM | Valid: [ 1/50] Step 750/781 Loss 1.338 Prec@(1,5) (53.1%, 94.0%)
06/28 10:50:16 PM | Valid: [ 1/50] Step 781/781 Loss 1.337 Prec@(1,5) (53.1%, 94.0%)
06/28 10:50:16 PM | Valid: [ 1/50] Final Prec@1 53.1440%
06/28 10:50:16 PM | genotype = Genotype(normal=[[('sep_conv_5x5', 1), ('sep_conv_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 3), ('sep_conv_3x3', 2)], [('dil_conv_5x5', 4), ('sep_conv_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 3)], [('max_pool_3x3', 0), ('sep_conv_5x5', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1266, 0.1190, 0.1251, 0.1273, 0.1255, 0.1271, 0.1250, 0.1243],
        [0.1229, 0.1182, 0.1220, 0.1250, 0.1274, 0.1270, 0.1263, 0.1310]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1264, 0.1190, 0.1232, 0.1264, 0.1272, 0.1238, 0.1269, 0.1271],
        [0.1239, 0.1180, 0.1220, 0.1261, 0.1272, 0.1268, 0.1275, 0.1284],
        [0.1191, 0.1158, 0.1218, 0.1256, 0.1250, 0.1288, 0.1301, 0.1338]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1260, 0.1177, 0.1221, 0.1274, 0.1278, 0.1252, 0.1264, 0.1276],
        [0.1224, 0.1171, 0.1201, 0.1276, 0.1289, 0.1282, 0.1275, 0.1283],
        [0.1186, 0.1157, 0.1213, 0.1286, 0.1283, 0.1280, 0.1268, 0.1328],
        [0.1176, 0.1145, 0.1187, 0.1296, 0.1269, 0.1296, 0.1294, 0.1336]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1243, 0.1172, 0.1205, 0.1278, 0.1265, 0.1268, 0.1276, 0.1294],
        [0.1227, 0.1178, 0.1208, 0.1273, 0.1291, 0.1261, 0.1272, 0.1289],
        [0.1171, 0.1145, 0.1197, 0.1268, 0.1286, 0.1305, 0.1285, 0.1344],
        [0.1161, 0.1138, 0.1168, 0.1319, 0.1284, 0.1299, 0.1301, 0.1330],
        [0.1161, 0.1139, 0.1165, 0.1289, 0.1310, 0.1288, 0.1318, 0.1328]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1281, 0.1244, 0.1242, 0.1252, 0.1275, 0.1226, 0.1252, 0.1229],
        [0.1256, 0.1207, 0.1265, 0.1238, 0.1257, 0.1255, 0.1256, 0.1266]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1290, 0.1245, 0.1255, 0.1264, 0.1256, 0.1232, 0.1229, 0.1228],
        [0.1271, 0.1223, 0.1258, 0.1255, 0.1249, 0.1248, 0.1227, 0.1269],
        [0.1255, 0.1199, 0.1255, 0.1251, 0.1253, 0.1257, 0.1272, 0.1258]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1282, 0.1233, 0.1270, 0.1258, 0.1250, 0.1244, 0.1236, 0.1227],
        [0.1262, 0.1217, 0.1237, 0.1248, 0.1287, 0.1277, 0.1233, 0.1238],
        [0.1245, 0.1184, 0.1239, 0.1243, 0.1278, 0.1253, 0.1295, 0.1263],
        [0.1217, 0.1184, 0.1243, 0.1244, 0.1290, 0.1284, 0.1254, 0.1283]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1290, 0.1247, 0.1232, 0.1270, 0.1241, 0.1247, 0.1234, 0.1239],
        [0.1268, 0.1221, 0.1258, 0.1246, 0.1243, 0.1264, 0.1261, 0.1240],
        [0.1244, 0.1201, 0.1250, 0.1242, 0.1290, 0.1239, 0.1277, 0.1257],
        [0.1220, 0.1187, 0.1245, 0.1287, 0.1272, 0.1257, 0.1254, 0.1279],
        [0.1225, 0.1204, 0.1245, 0.1257, 0.1253, 0.1266, 0.1281, 0.1270]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 10:50:22 PM | Train: [ 2/50] Step 000/781 Loss 1.193 Prec@(1,5) (56.2%, 100.0%)
06/28 10:54:06 PM | Train: [ 2/50] Step 050/781 Loss 1.307 Prec@(1,5) (53.6%, 93.6%)
06/28 10:57:50 PM | Train: [ 2/50] Step 100/781 Loss 1.253 Prec@(1,5) (55.6%, 94.3%)
06/28 11:01:34 PM | Train: [ 2/50] Step 150/781 Loss 1.245 Prec@(1,5) (55.5%, 94.6%)
06/28 11:05:18 PM | Train: [ 2/50] Step 200/781 Loss 1.241 Prec@(1,5) (55.4%, 94.4%)
06/28 11:09:02 PM | Train: [ 2/50] Step 250/781 Loss 1.235 Prec@(1,5) (55.6%, 94.6%)
06/28 11:12:46 PM | Train: [ 2/50] Step 300/781 Loss 1.224 Prec@(1,5) (56.0%, 94.8%)
06/28 11:16:30 PM | Train: [ 2/50] Step 350/781 Loss 1.210 Prec@(1,5) (56.6%, 95.0%)
06/28 11:20:15 PM | Train: [ 2/50] Step 400/781 Loss 1.198 Prec@(1,5) (56.9%, 95.1%)
06/28 11:23:59 PM | Train: [ 2/50] Step 450/781 Loss 1.196 Prec@(1,5) (57.1%, 95.2%)
06/28 11:27:41 PM | Train: [ 2/50] Step 500/781 Loss 1.186 Prec@(1,5) (57.6%, 95.2%)
06/28 11:31:21 PM | Train: [ 2/50] Step 550/781 Loss 1.180 Prec@(1,5) (58.0%, 95.2%)
06/28 11:35:01 PM | Train: [ 2/50] Step 600/781 Loss 1.172 Prec@(1,5) (58.3%, 95.3%)
06/28 11:38:42 PM | Train: [ 2/50] Step 650/781 Loss 1.159 Prec@(1,5) (58.8%, 95.4%)
06/28 11:42:22 PM | Train: [ 2/50] Step 700/781 Loss 1.152 Prec@(1,5) (59.0%, 95.5%)
06/28 11:46:02 PM | Train: [ 2/50] Step 750/781 Loss 1.144 Prec@(1,5) (59.4%, 95.5%)
06/28 11:48:17 PM | Train: [ 2/50] Step 781/781 Loss 1.140 Prec@(1,5) (59.5%, 95.5%)
06/28 11:48:17 PM | Train: [ 2/50] Final Prec@1 59.5280%
06/28 11:48:18 PM | Valid: [ 2/50] Step 000/781 Loss 0.708 Prec@(1,5) (81.2%, 96.9%)
06/28 11:48:28 PM | Valid: [ 2/50] Step 050/781 Loss 1.032 Prec@(1,5) (62.5%, 96.9%)
06/28 11:48:39 PM | Valid: [ 2/50] Step 100/781 Loss 1.023 Prec@(1,5) (63.1%, 96.7%)
06/28 11:48:49 PM | Valid: [ 2/50] Step 150/781 Loss 1.024 Prec@(1,5) (63.4%, 96.6%)
06/28 11:49:00 PM | Valid: [ 2/50] Step 200/781 Loss 1.037 Prec@(1,5) (63.2%, 96.4%)
06/28 11:49:11 PM | Valid: [ 2/50] Step 250/781 Loss 1.044 Prec@(1,5) (63.2%, 96.3%)
06/28 11:49:21 PM | Valid: [ 2/50] Step 300/781 Loss 1.037 Prec@(1,5) (63.5%, 96.3%)
06/28 11:49:32 PM | Valid: [ 2/50] Step 350/781 Loss 1.043 Prec@(1,5) (63.2%, 96.4%)
06/28 11:49:43 PM | Valid: [ 2/50] Step 400/781 Loss 1.046 Prec@(1,5) (63.3%, 96.3%)
06/28 11:49:53 PM | Valid: [ 2/50] Step 450/781 Loss 1.047 Prec@(1,5) (63.2%, 96.3%)
06/28 11:50:04 PM | Valid: [ 2/50] Step 500/781 Loss 1.048 Prec@(1,5) (63.2%, 96.3%)
06/28 11:50:15 PM | Valid: [ 2/50] Step 550/781 Loss 1.050 Prec@(1,5) (63.1%, 96.3%)
06/28 11:50:25 PM | Valid: [ 2/50] Step 600/781 Loss 1.054 Prec@(1,5) (63.1%, 96.3%)
06/28 11:50:36 PM | Valid: [ 2/50] Step 650/781 Loss 1.051 Prec@(1,5) (63.1%, 96.3%)
06/28 11:50:47 PM | Valid: [ 2/50] Step 700/781 Loss 1.047 Prec@(1,5) (63.2%, 96.4%)
06/28 11:50:57 PM | Valid: [ 2/50] Step 750/781 Loss 1.051 Prec@(1,5) (63.1%, 96.3%)
06/28 11:51:04 PM | Valid: [ 2/50] Step 781/781 Loss 1.050 Prec@(1,5) (63.2%, 96.3%)
06/28 11:51:04 PM | Valid: [ 2/50] Final Prec@1 63.2200%
06/28 11:51:04 PM | genotype = Genotype(normal=[[('dil_conv_3x3', 1), ('sep_conv_3x3', 0)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 0)], [('sep_conv_3x3', 3), ('dil_conv_3x3', 1)], [('sep_conv_5x5', 4), ('sep_conv_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('dil_conv_3x3', 3), ('dil_conv_5x5', 2)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1279, 0.1139, 0.1252, 0.1291, 0.1260, 0.1287, 0.1248, 0.1244],
        [0.1231, 0.1143, 0.1212, 0.1247, 0.1270, 0.1297, 0.1260, 0.1340]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1280, 0.1138, 0.1210, 0.1273, 0.1292, 0.1255, 0.1274, 0.1280],
        [0.1246, 0.1147, 0.1224, 0.1274, 0.1279, 0.1257, 0.1267, 0.1306],
        [0.1165, 0.1093, 0.1189, 0.1276, 0.1249, 0.1284, 0.1366, 0.1378]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1286, 0.1124, 0.1209, 0.1279, 0.1295, 0.1246, 0.1263, 0.1299],
        [0.1216, 0.1119, 0.1178, 0.1303, 0.1295, 0.1312, 0.1269, 0.1307],
        [0.1163, 0.1094, 0.1193, 0.1296, 0.1303, 0.1276, 0.1291, 0.1383],
        [0.1148, 0.1074, 0.1140, 0.1346, 0.1289, 0.1320, 0.1308, 0.1375]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1259, 0.1126, 0.1193, 0.1286, 0.1279, 0.1255, 0.1284, 0.1318],
        [0.1223, 0.1134, 0.1189, 0.1276, 0.1306, 0.1296, 0.1265, 0.1311],
        [0.1143, 0.1080, 0.1168, 0.1290, 0.1299, 0.1316, 0.1298, 0.1407],
        [0.1131, 0.1074, 0.1127, 0.1347, 0.1294, 0.1323, 0.1318, 0.1386],
        [0.1123, 0.1071, 0.1113, 0.1323, 0.1353, 0.1318, 0.1319, 0.1379]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1333, 0.1246, 0.1247, 0.1267, 0.1277, 0.1229, 0.1193, 0.1209],
        [0.1281, 0.1183, 0.1264, 0.1261, 0.1259, 0.1229, 0.1231, 0.1291]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1346, 0.1252, 0.1244, 0.1256, 0.1250, 0.1228, 0.1227, 0.1196],
        [0.1288, 0.1194, 0.1241, 0.1254, 0.1271, 0.1254, 0.1217, 0.1280],
        [0.1277, 0.1141, 0.1249, 0.1242, 0.1261, 0.1255, 0.1307, 0.1269]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1319, 0.1214, 0.1278, 0.1252, 0.1261, 0.1228, 0.1226, 0.1223],
        [0.1290, 0.1198, 0.1249, 0.1235, 0.1278, 0.1273, 0.1231, 0.1246],
        [0.1254, 0.1108, 0.1215, 0.1241, 0.1324, 0.1240, 0.1337, 0.1281],
        [0.1200, 0.1108, 0.1202, 0.1248, 0.1341, 0.1354, 0.1279, 0.1268]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1323, 0.1238, 0.1205, 0.1296, 0.1271, 0.1231, 0.1212, 0.1224],
        [0.1286, 0.1201, 0.1244, 0.1250, 0.1241, 0.1262, 0.1247, 0.1270],
        [0.1246, 0.1132, 0.1232, 0.1260, 0.1307, 0.1228, 0.1324, 0.1272],
        [0.1195, 0.1117, 0.1215, 0.1313, 0.1306, 0.1285, 0.1291, 0.1278],
        [0.1213, 0.1146, 0.1232, 0.1263, 0.1255, 0.1276, 0.1313, 0.1300]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
06/28 11:51:09 PM | Train: [ 3/50] Step 000/781 Loss 1.001 Prec@(1,5) (53.1%, 100.0%)
06/28 11:54:49 PM | Train: [ 3/50] Step 050/781 Loss 0.970 Prec@(1,5) (67.5%, 96.8%)
06/28 11:58:29 PM | Train: [ 3/50] Step 100/781 Loss 0.971 Prec@(1,5) (66.7%, 96.9%)
06/29 12:02:09 AM | Train: [ 3/50] Step 150/781 Loss 0.987 Prec@(1,5) (65.8%, 96.9%)
06/29 12:05:49 AM | Train: [ 3/50] Step 200/781 Loss 0.989 Prec@(1,5) (65.7%, 96.8%)
06/29 12:09:29 AM | Train: [ 3/50] Step 250/781 Loss 0.983 Prec@(1,5) (65.9%, 96.8%)
06/29 12:13:09 AM | Train: [ 3/50] Step 300/781 Loss 0.974 Prec@(1,5) (66.3%, 96.8%)
06/29 12:16:49 AM | Train: [ 3/50] Step 350/781 Loss 0.973 Prec@(1,5) (66.4%, 96.8%)
06/29 12:20:29 AM | Train: [ 3/50] Step 400/781 Loss 0.969 Prec@(1,5) (66.6%, 96.9%)
06/29 12:24:10 AM | Train: [ 3/50] Step 450/781 Loss 0.968 Prec@(1,5) (66.7%, 96.9%)
06/29 12:27:50 AM | Train: [ 3/50] Step 500/781 Loss 0.958 Prec@(1,5) (67.0%, 96.9%)
06/29 12:31:30 AM | Train: [ 3/50] Step 550/781 Loss 0.956 Prec@(1,5) (66.9%, 97.0%)
07/01 09:39:45 AM | 
07/01 09:39:45 AM | Parameters:
07/01 09:39:45 AM | ALPHA_LR=0.0003
07/01 09:39:45 AM | ALPHA_WEIGHT_DECAY=0.001
07/01 09:39:45 AM | BATCH_SIZE=32
07/01 09:39:45 AM | DATA_PATH=./data/
07/01 09:39:45 AM | DATASET=cifar10
07/01 09:39:45 AM | EPOCHS=1
07/01 09:39:45 AM | GPUS=[0]
07/01 09:39:45 AM | INIT_CHANNELS=16
07/01 09:39:45 AM | LAYERS=8
07/01 09:39:45 AM | NAME=cifar10
07/01 09:39:45 AM | PATH=searchs/cifar10
07/01 09:39:45 AM | PLOT_PATH=searchs/cifar10/plots
07/01 09:39:45 AM | PRINT_FREQ=50
07/01 09:39:45 AM | SEED=2
07/01 09:39:45 AM | W_GRAD_CLIP=5.0
07/01 09:39:45 AM | W_LR=0.025
07/01 09:39:45 AM | W_LR_MIN=0.001
07/01 09:39:45 AM | W_MOMENTUM=0.9
07/01 09:39:45 AM | W_WEIGHT_DECAY=0.0003
07/01 09:39:45 AM | WORKERS=4
07/01 09:39:45 AM | 
07/01 09:39:45 AM | Logger is set - training start
07/01 09:47:16 AM | 
07/01 09:47:16 AM | Parameters:
07/01 09:47:16 AM | ALPHA_LR=0.0003
07/01 09:47:16 AM | ALPHA_WEIGHT_DECAY=0.001
07/01 09:47:16 AM | BATCH_SIZE=32
07/01 09:47:16 AM | DATA_PATH=./data/
07/01 09:47:16 AM | DATASET=cifar10
07/01 09:47:16 AM | EPOCHS=1
07/01 09:47:16 AM | GPUS=[0]
07/01 09:47:16 AM | INIT_CHANNELS=16
07/01 09:47:16 AM | LAYERS=8
07/01 09:47:16 AM | NAME=cifar10
07/01 09:47:16 AM | PATH=searchs/cifar10
07/01 09:47:16 AM | PLOT_PATH=searchs/cifar10/plots
07/01 09:47:16 AM | PRINT_FREQ=50
07/01 09:47:16 AM | SEED=2
07/01 09:47:16 AM | W_GRAD_CLIP=5.0
07/01 09:47:16 AM | W_LR=0.025
07/01 09:47:16 AM | W_LR_MIN=0.001
07/01 09:47:16 AM | W_MOMENTUM=0.9
07/01 09:47:16 AM | W_WEIGHT_DECAY=0.0003
07/01 09:47:16 AM | WORKERS=4
07/01 09:47:16 AM | 
07/01 09:47:16 AM | Logger is set - training start
07/01 09:48:58 AM | 
07/01 09:48:58 AM | Parameters:
07/01 09:48:58 AM | ALPHA_LR=0.0003
07/01 09:48:58 AM | ALPHA_WEIGHT_DECAY=0.001
07/01 09:48:58 AM | BATCH_SIZE=32
07/01 09:48:58 AM | DATA_PATH=./data/
07/01 09:48:58 AM | DATASET=cifar10
07/01 09:48:58 AM | EPOCHS=1
07/01 09:48:58 AM | GPUS=[0]
07/01 09:48:58 AM | INIT_CHANNELS=16
07/01 09:48:58 AM | LAYERS=8
07/01 09:48:58 AM | NAME=cifar10
07/01 09:48:58 AM | PATH=searchs/cifar10
07/01 09:48:58 AM | PLOT_PATH=searchs/cifar10/plots
07/01 09:48:58 AM | PRINT_FREQ=50
07/01 09:48:58 AM | SEED=2
07/01 09:48:58 AM | W_GRAD_CLIP=5.0
07/01 09:48:58 AM | W_LR=0.025
07/01 09:48:58 AM | W_LR_MIN=0.001
07/01 09:48:58 AM | W_MOMENTUM=0.9
07/01 09:48:58 AM | W_WEIGHT_DECAY=0.0003
07/01 09:48:58 AM | WORKERS=4
07/01 09:48:58 AM | 
07/01 09:48:58 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/01 09:49:09 AM | Train: [ 1/1] Step 000/781 Loss 2.312 Prec@(1,5) (15.6%, 56.2%)
07/01 09:51:21 AM | Train: [ 1/1] Step 050/781 Loss 2.171 Prec@(1,5) (22.1%, 75.9%)
07/01 09:53:33 AM | Train: [ 1/1] Step 100/781 Loss 2.088 Prec@(1,5) (24.8%, 79.3%)
07/01 09:55:46 AM | Train: [ 1/1] Step 150/781 Loss 2.007 Prec@(1,5) (26.7%, 80.7%)
07/01 09:57:58 AM | Train: [ 1/1] Step 200/781 Loss 1.941 Prec@(1,5) (28.3%, 82.6%)
07/01 10:00:10 AM | Train: [ 1/1] Step 250/781 Loss 1.888 Prec@(1,5) (30.0%, 83.8%)
07/01 10:02:20 AM | Train: [ 1/1] Step 300/781 Loss 1.858 Prec@(1,5) (31.0%, 84.6%)
07/01 10:04:29 AM | Train: [ 1/1] Step 350/781 Loss 1.827 Prec@(1,5) (32.0%, 85.1%)
07/01 10:06:40 AM | Train: [ 1/1] Step 400/781 Loss 1.791 Prec@(1,5) (33.5%, 86.0%)
07/01 10:08:55 AM | Train: [ 1/1] Step 450/781 Loss 1.760 Prec@(1,5) (34.9%, 86.6%)
07/01 10:11:13 AM | Train: [ 1/1] Step 500/781 Loss 1.730 Prec@(1,5) (36.0%, 87.2%)
07/01 10:13:25 AM | Train: [ 1/1] Step 550/781 Loss 1.702 Prec@(1,5) (37.1%, 87.6%)
07/01 10:15:43 AM | Train: [ 1/1] Step 600/781 Loss 1.680 Prec@(1,5) (38.0%, 88.0%)
07/01 10:17:58 AM | Train: [ 1/1] Step 650/781 Loss 1.660 Prec@(1,5) (38.9%, 88.2%)
07/01 10:20:10 AM | Train: [ 1/1] Step 700/781 Loss 1.639 Prec@(1,5) (39.8%, 88.6%)
07/01 10:22:22 AM | Train: [ 1/1] Step 750/781 Loss 1.623 Prec@(1,5) (40.4%, 88.8%)
07/01 10:23:47 AM | Train: [ 1/1] Step 781/781 Loss 1.613 Prec@(1,5) (40.8%, 89.1%)
07/01 10:23:48 AM | Train: [ 1/1] Final Prec@1 40.8040%
07/01 10:23:48 AM | Valid: [ 1/1] Step 000/781 Loss 1.304 Prec@(1,5) (53.1%, 93.8%)
07/01 10:23:54 AM | Valid: [ 1/1] Step 050/781 Loss 1.247 Prec@(1,5) (54.3%, 94.8%)
07/01 10:24:01 AM | Valid: [ 1/1] Step 100/781 Loss 1.292 Prec@(1,5) (53.4%, 94.2%)
07/01 10:24:07 AM | Valid: [ 1/1] Step 150/781 Loss 1.298 Prec@(1,5) (53.4%, 93.9%)
07/01 10:24:13 AM | Valid: [ 1/1] Step 200/781 Loss 1.290 Prec@(1,5) (53.5%, 94.2%)
07/01 10:24:19 AM | Valid: [ 1/1] Step 250/781 Loss 1.301 Prec@(1,5) (53.1%, 94.2%)
07/01 10:24:26 AM | Valid: [ 1/1] Step 300/781 Loss 1.308 Prec@(1,5) (53.1%, 94.1%)
07/01 10:24:32 AM | Valid: [ 1/1] Step 350/781 Loss 1.311 Prec@(1,5) (52.9%, 93.9%)
07/01 10:24:38 AM | Valid: [ 1/1] Step 400/781 Loss 1.313 Prec@(1,5) (52.8%, 93.8%)
07/01 10:24:44 AM | Valid: [ 1/1] Step 450/781 Loss 1.314 Prec@(1,5) (52.9%, 93.7%)
07/01 10:24:50 AM | Valid: [ 1/1] Step 500/781 Loss 1.311 Prec@(1,5) (52.9%, 93.8%)
07/01 10:24:57 AM | Valid: [ 1/1] Step 550/781 Loss 1.310 Prec@(1,5) (53.0%, 93.8%)
07/01 10:25:04 AM | Valid: [ 1/1] Step 600/781 Loss 1.315 Prec@(1,5) (52.9%, 93.8%)
07/01 10:25:11 AM | Valid: [ 1/1] Step 650/781 Loss 1.317 Prec@(1,5) (52.8%, 93.8%)
07/01 10:25:19 AM | Valid: [ 1/1] Step 700/781 Loss 1.317 Prec@(1,5) (52.9%, 93.8%)
07/01 10:25:26 AM | Valid: [ 1/1] Step 750/781 Loss 1.316 Prec@(1,5) (53.1%, 93.7%)
07/01 10:25:31 AM | Valid: [ 1/1] Step 781/781 Loss 1.317 Prec@(1,5) (53.0%, 93.7%)
07/01 10:25:31 AM | Valid: [ 1/1] Final Prec@1 53.0040%
07/01 10:25:31 AM | genotype = Genotype(normal=[[('dil_conv_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_5x5', 1), ('dil_conv_3x3', 2)], [('dil_conv_3x3', 3), ('sep_conv_5x5', 2)], [('sep_conv_5x5', 2), ('sep_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('dil_conv_5x5', 2), ('max_pool_3x3', 0)], [('sep_conv_5x5', 3), ('dil_conv_5x5', 2)], [('max_pool_3x3', 0), ('dil_conv_5x5', 4)]], reduce_concat=range(2, 6))
07/01 11:13:23 AM | 
07/01 11:13:23 AM | Parameters:
07/01 11:13:23 AM | ALPHA_LR=0.0003
07/01 11:13:23 AM | ALPHA_WEIGHT_DECAY=0.001
07/01 11:13:23 AM | BATCH_SIZE=32
07/01 11:13:23 AM | DATA_PATH=./data/
07/01 11:13:23 AM | DATASET=cifar10
07/01 11:13:23 AM | EPOCHS=1
07/01 11:13:23 AM | GPUS=[0]
07/01 11:13:23 AM | INIT_CHANNELS=16
07/01 11:13:23 AM | LAYERS=8
07/01 11:13:23 AM | NAME=cifar10
07/01 11:13:23 AM | PATH=searchs/cifar10
07/01 11:13:23 AM | PLOT_PATH=searchs/cifar10/plots
07/01 11:13:23 AM | PRINT_FREQ=50
07/01 11:13:23 AM | SEED=2
07/01 11:13:23 AM | W_GRAD_CLIP=5.0
07/01 11:13:23 AM | W_LR=0.025
07/01 11:13:23 AM | W_LR_MIN=0.001
07/01 11:13:23 AM | W_MOMENTUM=0.9
07/01 11:13:23 AM | W_WEIGHT_DECAY=0.0003
07/01 11:13:23 AM | WORKERS=4
07/01 11:13:23 AM | 
07/01 11:13:23 AM | Logger is set - training start
07/01 11:43:34 AM | 
07/01 11:43:34 AM | Parameters:
07/01 11:43:34 AM | ALPHA_LR=0.0003
07/01 11:43:34 AM | ALPHA_WEIGHT_DECAY=0.001
07/01 11:43:34 AM | BATCH_SIZE=32
07/01 11:43:34 AM | DATA_PATH=./data/
07/01 11:43:34 AM | DATASET=cifar10
07/01 11:43:34 AM | EPOCHS=1
07/01 11:43:34 AM | GPUS=[0]
07/01 11:43:34 AM | INIT_CHANNELS=16
07/01 11:43:34 AM | LAYERS=8
07/01 11:43:34 AM | NAME=cifar10
07/01 11:43:34 AM | PATH=searchs/cifar10
07/01 11:43:34 AM | PLOT_PATH=searchs/cifar10/plots
07/01 11:43:34 AM | PRINT_FREQ=50
07/01 11:43:34 AM | SEED=2
07/01 11:43:34 AM | W_GRAD_CLIP=5.0
07/01 11:43:34 AM | W_LR=0.025
07/01 11:43:34 AM | W_LR_MIN=0.001
07/01 11:43:34 AM | W_MOMENTUM=0.9
07/01 11:43:34 AM | W_WEIGHT_DECAY=0.0003
07/01 11:43:34 AM | WORKERS=4
07/01 11:43:34 AM | 
07/01 11:43:34 AM | Logger is set - training start
07/01 11:51:06 AM | 
07/01 11:51:06 AM | Parameters:
07/01 11:51:06 AM | ALPHA_LR=0.0003
07/01 11:51:06 AM | ALPHA_WEIGHT_DECAY=0.001
07/01 11:51:06 AM | BATCH_SIZE=32
07/01 11:51:06 AM | DATA_PATH=./data/
07/01 11:51:06 AM | DATASET=cifar10
07/01 11:51:06 AM | EPOCHS=1
07/01 11:51:06 AM | GPUS=[0]
07/01 11:51:06 AM | INIT_CHANNELS=16
07/01 11:51:06 AM | LAYERS=8
07/01 11:51:06 AM | NAME=cifar10
07/01 11:51:06 AM | PATH=searchs/cifar10
07/01 11:51:06 AM | PLOT_PATH=searchs/cifar10/plots
07/01 11:51:06 AM | PRINT_FREQ=50
07/01 11:51:06 AM | SEED=2
07/01 11:51:06 AM | W_GRAD_CLIP=5.0
07/01 11:51:06 AM | W_LR=0.025
07/01 11:51:06 AM | W_LR_MIN=0.001
07/01 11:51:06 AM | W_MOMENTUM=0.9
07/01 11:51:06 AM | W_WEIGHT_DECAY=0.0003
07/01 11:51:06 AM | WORKERS=4
07/01 11:51:06 AM | 
07/01 11:51:06 AM | Logger is set - training start
07/01 11:58:09 AM | 
07/01 11:58:09 AM | Parameters:
07/01 11:58:09 AM | ALPHA_LR=0.0003
07/01 11:58:09 AM | ALPHA_WEIGHT_DECAY=0.001
07/01 11:58:09 AM | BATCH_SIZE=32
07/01 11:58:09 AM | DATA_PATH=./data/
07/01 11:58:09 AM | DATASET=cifar10
07/01 11:58:09 AM | EPOCHS=1
07/01 11:58:09 AM | GPUS=[0]
07/01 11:58:09 AM | INIT_CHANNELS=16
07/01 11:58:09 AM | LAYERS=8
07/01 11:58:09 AM | NAME=cifar10
07/01 11:58:09 AM | PATH=searchs/cifar10
07/01 11:58:09 AM | PLOT_PATH=searchs/cifar10/plots
07/01 11:58:09 AM | PRINT_FREQ=50
07/01 11:58:09 AM | SEED=2
07/01 11:58:09 AM | W_GRAD_CLIP=5.0
07/01 11:58:09 AM | W_LR=0.025
07/01 11:58:09 AM | W_LR_MIN=0.001
07/01 11:58:09 AM | W_MOMENTUM=0.9
07/01 11:58:09 AM | W_WEIGHT_DECAY=0.0003
07/01 11:58:09 AM | WORKERS=4
07/01 11:58:09 AM | 
07/01 11:58:09 AM | Logger is set - training start
07/01 12:05:15 PM | 
07/01 12:05:15 PM | Parameters:
07/01 12:05:15 PM | ALPHA_LR=0.0003
07/01 12:05:15 PM | ALPHA_WEIGHT_DECAY=0.001
07/01 12:05:15 PM | BATCH_SIZE=32
07/01 12:05:15 PM | DATA_PATH=./data/
07/01 12:05:15 PM | DATASET=cifar10
07/01 12:05:15 PM | EPOCHS=1
07/01 12:05:15 PM | GPUS=[0]
07/01 12:05:15 PM | INIT_CHANNELS=16
07/01 12:05:15 PM | LAYERS=8
07/01 12:05:15 PM | NAME=cifar10
07/01 12:05:15 PM | PATH=searchs/cifar10
07/01 12:05:15 PM | PLOT_PATH=searchs/cifar10/plots
07/01 12:05:15 PM | PRINT_FREQ=50
07/01 12:05:15 PM | SEED=2
07/01 12:05:15 PM | W_GRAD_CLIP=5.0
07/01 12:05:15 PM | W_LR=0.025
07/01 12:05:15 PM | W_LR_MIN=0.001
07/01 12:05:15 PM | W_MOMENTUM=0.9
07/01 12:05:15 PM | W_WEIGHT_DECAY=0.0003
07/01 12:05:15 PM | WORKERS=4
07/01 12:05:15 PM | 
07/01 12:05:15 PM | Logger is set - training start
07/01 12:06:05 PM | 
07/01 12:06:05 PM | Parameters:
07/01 12:06:05 PM | ALPHA_LR=0.0003
07/01 12:06:05 PM | ALPHA_WEIGHT_DECAY=0.001
07/01 12:06:05 PM | BATCH_SIZE=32
07/01 12:06:05 PM | DATA_PATH=./data/
07/01 12:06:05 PM | DATASET=cifar10
07/01 12:06:05 PM | EPOCHS=1
07/01 12:06:05 PM | GPUS=[0]
07/01 12:06:05 PM | INIT_CHANNELS=16
07/01 12:06:05 PM | LAYERS=8
07/01 12:06:05 PM | NAME=cifar10
07/01 12:06:05 PM | PATH=searchs/cifar10
07/01 12:06:05 PM | PLOT_PATH=searchs/cifar10/plots
07/01 12:06:05 PM | PRINT_FREQ=50
07/01 12:06:05 PM | SEED=2
07/01 12:06:05 PM | W_GRAD_CLIP=5.0
07/01 12:06:05 PM | W_LR=0.025
07/01 12:06:05 PM | W_LR_MIN=0.001
07/01 12:06:05 PM | W_MOMENTUM=0.9
07/01 12:06:05 PM | W_WEIGHT_DECAY=0.0003
07/01 12:06:05 PM | WORKERS=4
07/01 12:06:05 PM | 
07/01 12:06:05 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/01 12:06:18 PM | Train: [ 1/1] Step 000/781 Loss 2.258 Prec@(1,5) (25.0%, 75.0%)
07/01 12:08:43 PM | Train: [ 1/1] Step 050/781 Loss 2.142 Prec@(1,5) (20.5%, 70.3%)
07/01 12:11:08 PM | Train: [ 1/1] Step 100/781 Loss 2.008 Prec@(1,5) (25.1%, 77.1%)
07/01 12:13:33 PM | Train: [ 1/1] Step 150/781 Loss 1.933 Prec@(1,5) (27.6%, 80.2%)
07/01 12:15:59 PM | Train: [ 1/1] Step 200/781 Loss 1.879 Prec@(1,5) (29.5%, 82.4%)
07/01 12:18:24 PM | Train: [ 1/1] Step 250/781 Loss 1.835 Prec@(1,5) (31.0%, 83.8%)
07/01 12:20:50 PM | Train: [ 1/1] Step 300/781 Loss 1.805 Prec@(1,5) (32.3%, 84.7%)
07/01 12:23:16 PM | Train: [ 1/1] Step 350/781 Loss 1.782 Prec@(1,5) (33.4%, 85.2%)
07/01 12:25:39 PM | Train: [ 1/1] Step 400/781 Loss 1.756 Prec@(1,5) (34.4%, 85.9%)
07/01 12:28:03 PM | Train: [ 1/1] Step 450/781 Loss 1.734 Prec@(1,5) (35.3%, 86.5%)
07/01 12:30:26 PM | Train: [ 1/1] Step 500/781 Loss 1.716 Prec@(1,5) (36.2%, 86.9%)
07/01 12:32:49 PM | Train: [ 1/1] Step 550/781 Loss 1.700 Prec@(1,5) (36.9%, 87.3%)
07/01 12:35:14 PM | Train: [ 1/1] Step 600/781 Loss 1.683 Prec@(1,5) (37.7%, 87.6%)
07/01 12:37:39 PM | Train: [ 1/1] Step 650/781 Loss 1.668 Prec@(1,5) (38.3%, 87.9%)
07/01 12:40:02 PM | Train: [ 1/1] Step 700/781 Loss 1.653 Prec@(1,5) (38.9%, 88.2%)
07/01 12:42:24 PM | Train: [ 1/1] Step 750/781 Loss 1.641 Prec@(1,5) (39.5%, 88.4%)
07/01 12:43:51 PM | Train: [ 1/1] Step 781/781 Loss 1.633 Prec@(1,5) (39.8%, 88.5%)
07/01 12:43:51 PM | Train: [ 1/1] Final Prec@1 39.8400%
07/01 12:43:52 PM | Valid: [ 1/1] Step 000/781 Loss 1.383 Prec@(1,5) (53.1%, 87.5%)
07/01 12:44:10 PM | Valid: [ 1/1] Step 050/781 Loss 1.512 Prec@(1,5) (46.4%, 91.9%)
07/01 12:44:28 PM | Valid: [ 1/1] Step 100/781 Loss 1.492 Prec@(1,5) (46.3%, 92.2%)
07/01 12:44:46 PM | Valid: [ 1/1] Step 150/781 Loss 1.494 Prec@(1,5) (46.3%, 92.2%)
07/01 12:45:05 PM | Valid: [ 1/1] Step 200/781 Loss 1.492 Prec@(1,5) (46.0%, 92.4%)
07/01 12:45:23 PM | Valid: [ 1/1] Step 250/781 Loss 1.482 Prec@(1,5) (46.4%, 92.5%)
07/01 12:45:41 PM | Valid: [ 1/1] Step 300/781 Loss 1.481 Prec@(1,5) (46.6%, 92.4%)
07/01 12:46:00 PM | Valid: [ 1/1] Step 350/781 Loss 1.484 Prec@(1,5) (46.5%, 92.3%)
07/01 12:46:18 PM | Valid: [ 1/1] Step 400/781 Loss 1.482 Prec@(1,5) (46.4%, 92.4%)
07/01 12:46:36 PM | Valid: [ 1/1] Step 450/781 Loss 1.482 Prec@(1,5) (46.3%, 92.4%)
07/01 12:46:55 PM | Valid: [ 1/1] Step 500/781 Loss 1.481 Prec@(1,5) (46.4%, 92.5%)
07/01 12:47:13 PM | Valid: [ 1/1] Step 550/781 Loss 1.478 Prec@(1,5) (46.4%, 92.5%)
07/01 12:47:32 PM | Valid: [ 1/1] Step 600/781 Loss 1.479 Prec@(1,5) (46.2%, 92.6%)
07/01 12:47:50 PM | Valid: [ 1/1] Step 650/781 Loss 1.478 Prec@(1,5) (46.3%, 92.5%)
07/01 12:48:08 PM | Valid: [ 1/1] Step 700/781 Loss 1.475 Prec@(1,5) (46.4%, 92.6%)
07/01 12:48:26 PM | Valid: [ 1/1] Step 750/781 Loss 1.472 Prec@(1,5) (46.6%, 92.6%)
07/01 12:48:35 PM | Valid: [ 1/1] Step 781/781 Loss 1.471 Prec@(1,5) (46.5%, 92.7%)
07/01 12:48:35 PM | Valid: [ 1/1] Final Prec@1 46.5480%
07/01 12:48:35 PM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('sep_conv_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 0), ('dil_conv_3x3', 1)], [('sep_conv_5x5', 0), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('dil_conv_3x3', 2)], [('dil_conv_5x5', 4), ('skip_connect', 1)]], reduce_concat=range(2, 6))
07/01 02:11:43 PM | 
07/01 02:11:43 PM | Parameters:
07/01 02:11:43 PM | ALPHA_LR=0.0003
07/01 02:11:43 PM | ALPHA_WEIGHT_DECAY=0.001
07/01 02:11:43 PM | BATCH_SIZE=32
07/01 02:11:43 PM | DATA_PATH=./data/
07/01 02:11:43 PM | DATASET=cifar10
07/01 02:11:43 PM | EPOCHS=1
07/01 02:11:43 PM | GPUS=[0]
07/01 02:11:43 PM | INIT_CHANNELS=16
07/01 02:11:43 PM | LAYERS=8
07/01 02:11:43 PM | NAME=cifar10
07/01 02:11:43 PM | PATH=searchs/cifar10
07/01 02:11:43 PM | PLOT_PATH=searchs/cifar10/plots
07/01 02:11:43 PM | PRINT_FREQ=50
07/01 02:11:43 PM | SEED=2
07/01 02:11:43 PM | W_GRAD_CLIP=5.0
07/01 02:11:43 PM | W_LR=0.025
07/01 02:11:43 PM | W_LR_MIN=0.001
07/01 02:11:43 PM | W_MOMENTUM=0.9
07/01 02:11:43 PM | W_WEIGHT_DECAY=0.0003
07/01 02:11:43 PM | WORKERS=4
07/01 02:11:43 PM | 
07/01 02:11:43 PM | Logger is set - training start
07/01 02:11:59 PM | 
07/01 02:11:59 PM | Parameters:
07/01 02:11:59 PM | ALPHA_LR=0.0003
07/01 02:11:59 PM | ALPHA_WEIGHT_DECAY=0.001
07/01 02:11:59 PM | BATCH_SIZE=32
07/01 02:11:59 PM | DATA_PATH=./data/
07/01 02:11:59 PM | DATASET=cifar10
07/01 02:11:59 PM | EPOCHS=1
07/01 02:11:59 PM | GPUS=[0]
07/01 02:11:59 PM | INIT_CHANNELS=16
07/01 02:11:59 PM | LAYERS=8
07/01 02:11:59 PM | NAME=cifar10
07/01 02:11:59 PM | PATH=searchs/cifar10
07/01 02:11:59 PM | PLOT_PATH=searchs/cifar10/plots
07/01 02:11:59 PM | PRINT_FREQ=50
07/01 02:11:59 PM | SEED=2
07/01 02:11:59 PM | W_GRAD_CLIP=5.0
07/01 02:11:59 PM | W_LR=0.025
07/01 02:11:59 PM | W_LR_MIN=0.001
07/01 02:11:59 PM | W_MOMENTUM=0.9
07/01 02:11:59 PM | W_WEIGHT_DECAY=0.0003
07/01 02:11:59 PM | WORKERS=4
07/01 02:11:59 PM | 
07/01 02:11:59 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/01 02:12:10 PM | Train: [ 1/1] Step 000/781 Loss 2.258 Prec@(1,5) (25.0%, 75.0%)
07/01 02:14:34 PM | Train: [ 1/1] Step 050/781 Loss 2.141 Prec@(1,5) (20.5%, 70.3%)
07/01 02:16:56 PM | Train: [ 1/1] Step 100/781 Loss 2.006 Prec@(1,5) (25.3%, 77.5%)
07/01 02:19:20 PM | Train: [ 1/1] Step 150/781 Loss 1.933 Prec@(1,5) (27.9%, 80.6%)
07/01 02:21:43 PM | Train: [ 1/1] Step 200/781 Loss 1.879 Prec@(1,5) (29.8%, 82.4%)
07/01 02:24:03 PM | Train: [ 1/1] Step 250/781 Loss 1.835 Prec@(1,5) (31.2%, 84.0%)
07/01 02:26:22 PM | Train: [ 1/1] Step 300/781 Loss 1.805 Prec@(1,5) (32.6%, 84.8%)
07/01 02:28:43 PM | Train: [ 1/1] Step 350/781 Loss 1.783 Prec@(1,5) (33.3%, 85.4%)
07/01 02:31:06 PM | Train: [ 1/1] Step 400/781 Loss 1.756 Prec@(1,5) (34.3%, 86.1%)
07/01 02:33:30 PM | Train: [ 1/1] Step 450/781 Loss 1.735 Prec@(1,5) (35.1%, 86.7%)
07/01 02:35:50 PM | Train: [ 1/1] Step 500/781 Loss 1.717 Prec@(1,5) (35.9%, 87.1%)
07/01 02:38:11 PM | Train: [ 1/1] Step 550/781 Loss 1.702 Prec@(1,5) (36.7%, 87.4%)
07/01 02:40:31 PM | Train: [ 1/1] Step 600/781 Loss 1.685 Prec@(1,5) (37.4%, 87.8%)
07/01 02:42:52 PM | Train: [ 1/1] Step 650/781 Loss 1.670 Prec@(1,5) (37.9%, 88.1%)
07/01 02:45:15 PM | Train: [ 1/1] Step 700/781 Loss 1.656 Prec@(1,5) (38.6%, 88.3%)
07/01 02:47:36 PM | Train: [ 1/1] Step 750/781 Loss 1.643 Prec@(1,5) (39.2%, 88.5%)
07/01 02:49:04 PM | Train: [ 1/1] Step 781/781 Loss 1.635 Prec@(1,5) (39.5%, 88.7%)
07/01 02:49:04 PM | Train: [ 1/1] Final Prec@1 39.5240%
07/01 02:49:05 PM | Valid: [ 1/1] Step 000/781 Loss 1.420 Prec@(1,5) (53.1%, 93.8%)
07/01 02:49:23 PM | Valid: [ 1/1] Step 050/781 Loss 1.485 Prec@(1,5) (47.1%, 92.5%)
07/01 02:49:42 PM | Valid: [ 1/1] Step 100/781 Loss 1.472 Prec@(1,5) (46.6%, 93.1%)
07/01 02:50:00 PM | Valid: [ 1/1] Step 150/781 Loss 1.473 Prec@(1,5) (46.5%, 92.9%)
07/01 02:50:18 PM | Valid: [ 1/1] Step 200/781 Loss 1.475 Prec@(1,5) (46.7%, 92.9%)
07/01 02:50:37 PM | Valid: [ 1/1] Step 250/781 Loss 1.468 Prec@(1,5) (46.8%, 92.9%)
07/01 02:50:55 PM | Valid: [ 1/1] Step 300/781 Loss 1.467 Prec@(1,5) (46.9%, 92.8%)
07/01 02:51:14 PM | Valid: [ 1/1] Step 350/781 Loss 1.469 Prec@(1,5) (46.9%, 92.8%)
07/01 02:51:32 PM | Valid: [ 1/1] Step 400/781 Loss 1.469 Prec@(1,5) (46.7%, 92.8%)
07/01 02:51:50 PM | Valid: [ 1/1] Step 450/781 Loss 1.470 Prec@(1,5) (46.7%, 92.7%)
07/01 02:52:09 PM | Valid: [ 1/1] Step 500/781 Loss 1.470 Prec@(1,5) (46.7%, 92.7%)
07/01 02:52:27 PM | Valid: [ 1/1] Step 550/781 Loss 1.467 Prec@(1,5) (46.8%, 92.7%)
07/01 02:52:46 PM | Valid: [ 1/1] Step 600/781 Loss 1.467 Prec@(1,5) (46.7%, 92.7%)
07/01 02:53:04 PM | Valid: [ 1/1] Step 650/781 Loss 1.466 Prec@(1,5) (46.8%, 92.6%)
07/01 02:53:22 PM | Valid: [ 1/1] Step 700/781 Loss 1.463 Prec@(1,5) (46.9%, 92.6%)
07/01 02:53:40 PM | Valid: [ 1/1] Step 750/781 Loss 1.461 Prec@(1,5) (47.1%, 92.6%)
07/01 02:53:49 PM | Valid: [ 1/1] Step 781/781 Loss 1.459 Prec@(1,5) (47.0%, 92.7%)
07/01 02:53:50 PM | Valid: [ 1/1] Final Prec@1 47.0440%
07/01 02:53:50 PM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('skip_connect', 3)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], normal_concat=range(2, 6), reduce=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('sep_conv_5x5', 0), ('max_pool_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 3)], [('skip_connect', 1), ('dil_conv_5x5', 4)]], reduce_concat=range(2, 6))
07/01 02:53:50 PM | Final best Prec@1 = 47.0440%
07/01 02:53:50 PM | Best Genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('skip_connect', 3)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], normal_concat=range(2, 6), reduce=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('sep_conv_5x5', 0), ('max_pool_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 3)], [('skip_connect', 1), ('dil_conv_5x5', 4)]], reduce_concat=range(2, 6))
07/01 03:20:22 PM | 
07/01 03:20:22 PM | Parameters:
07/01 03:20:22 PM | ALPHA_LR=0.0003
07/01 03:20:22 PM | ALPHA_WEIGHT_DECAY=0.001
07/01 03:20:22 PM | BATCH_SIZE=32
07/01 03:20:22 PM | DATA_PATH=./data/
07/01 03:20:22 PM | DATASET=cifar10
07/01 03:20:22 PM | EPOCHS=1
07/01 03:20:22 PM | GPUS=[0]
07/01 03:20:22 PM | INIT_CHANNELS=16
07/01 03:20:22 PM | LAYERS=8
07/01 03:20:22 PM | NAME=cifar10
07/01 03:20:22 PM | PATH=searchs/cifar10
07/01 03:20:22 PM | PLOT_PATH=searchs/cifar10/plots
07/01 03:20:22 PM | PRINT_FREQ=50
07/01 03:20:22 PM | SEED=2
07/01 03:20:22 PM | W_GRAD_CLIP=5.0
07/01 03:20:22 PM | W_LR=0.025
07/01 03:20:22 PM | W_LR_MIN=0.001
07/01 03:20:22 PM | W_MOMENTUM=0.9
07/01 03:20:22 PM | W_WEIGHT_DECAY=0.0003
07/01 03:20:22 PM | WORKERS=4
07/01 03:20:22 PM | 
07/01 03:20:22 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/01 03:21:40 PM | 
07/01 03:21:40 PM | Parameters:
07/01 03:21:40 PM | ALPHA_LR=0.0003
07/01 03:21:40 PM | ALPHA_WEIGHT_DECAY=0.001
07/01 03:21:40 PM | BATCH_SIZE=32
07/01 03:21:40 PM | DATA_PATH=./data/
07/01 03:21:40 PM | DATASET=cifar10
07/01 03:21:40 PM | EPOCHS=1
07/01 03:21:40 PM | GPUS=[0]
07/01 03:21:40 PM | INIT_CHANNELS=16
07/01 03:21:40 PM | LAYERS=8
07/01 03:21:40 PM | NAME=cifar10
07/01 03:21:40 PM | PATH=searchs/cifar10
07/01 03:21:40 PM | PLOT_PATH=searchs/cifar10/plots
07/01 03:21:40 PM | PRINT_FREQ=50
07/01 03:21:40 PM | SEED=2
07/01 03:21:40 PM | W_GRAD_CLIP=5.0
07/01 03:21:40 PM | W_LR=0.025
07/01 03:21:40 PM | W_LR_MIN=0.001
07/01 03:21:40 PM | W_MOMENTUM=0.9
07/01 03:21:40 PM | W_WEIGHT_DECAY=0.0003
07/01 03:21:40 PM | WORKERS=4
07/01 03:21:40 PM | 
07/01 03:21:40 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/01 03:24:03 PM | 
07/01 03:24:03 PM | Parameters:
07/01 03:24:03 PM | ALPHA_LR=0.0003
07/01 03:24:03 PM | ALPHA_WEIGHT_DECAY=0.001
07/01 03:24:03 PM | BATCH_SIZE=32
07/01 03:24:03 PM | DATA_PATH=./data/
07/01 03:24:03 PM | DATASET=cifar10
07/01 03:24:03 PM | EPOCHS=1
07/01 03:24:03 PM | GPUS=[0]
07/01 03:24:03 PM | INIT_CHANNELS=16
07/01 03:24:03 PM | LAYERS=8
07/01 03:24:03 PM | NAME=cifar10
07/01 03:24:03 PM | PATH=searchs/cifar10
07/01 03:24:03 PM | PLOT_PATH=searchs/cifar10/plots
07/01 03:24:03 PM | PRINT_FREQ=50
07/01 03:24:03 PM | SEED=2
07/01 03:24:03 PM | W_GRAD_CLIP=5.0
07/01 03:24:03 PM | W_LR=0.025
07/01 03:24:03 PM | W_LR_MIN=0.001
07/01 03:24:03 PM | W_MOMENTUM=0.9
07/01 03:24:03 PM | W_WEIGHT_DECAY=0.0003
07/01 03:24:03 PM | WORKERS=4
07/01 03:24:03 PM | 
07/01 03:24:03 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/01 03:24:16 PM | Train: [ 1/1] Step 000/781 Loss 2.623 Prec@(1,5) (9.4%, 34.4%)
07/01 03:29:11 PM | Train: [ 1/1] Step 050/781 Loss 2.427 Prec@(1,5) (13.1%, 55.5%)
07/01 03:34:07 PM | Train: [ 1/1] Step 100/781 Loss 2.334 Prec@(1,5) (14.9%, 60.4%)
07/01 03:39:08 PM | Train: [ 1/1] Step 150/781 Loss 2.272 Prec@(1,5) (17.6%, 62.9%)
07/01 03:44:13 PM | Train: [ 1/1] Step 200/781 Loss 2.208 Prec@(1,5) (20.0%, 66.2%)
07/01 03:49:15 PM | Train: [ 1/1] Step 250/781 Loss 2.142 Prec@(1,5) (22.4%, 69.3%)
07/01 03:54:20 PM | Train: [ 1/1] Step 300/781 Loss 2.077 Prec@(1,5) (25.1%, 72.1%)
07/01 03:59:21 PM | Train: [ 1/1] Step 350/781 Loss 2.015 Prec@(1,5) (27.4%, 74.4%)
07/01 04:04:27 PM | Train: [ 1/1] Step 400/781 Loss 1.956 Prec@(1,5) (29.7%, 76.3%)
07/01 04:09:25 PM | Train: [ 1/1] Step 450/781 Loss 1.908 Prec@(1,5) (31.5%, 77.9%)
07/01 04:14:23 PM | Train: [ 1/1] Step 500/781 Loss 1.859 Prec@(1,5) (33.3%, 79.4%)
07/01 04:19:28 PM | Train: [ 1/1] Step 550/781 Loss 1.815 Prec@(1,5) (35.0%, 80.6%)
07/01 04:24:33 PM | Train: [ 1/1] Step 600/781 Loss 1.777 Prec@(1,5) (36.5%, 81.6%)
07/01 04:29:38 PM | Train: [ 1/1] Step 650/781 Loss 1.743 Prec@(1,5) (37.8%, 82.4%)
07/01 04:34:39 PM | Train: [ 1/1] Step 700/781 Loss 1.708 Prec@(1,5) (39.0%, 83.2%)
07/01 04:39:40 PM | Train: [ 1/1] Step 750/781 Loss 1.674 Prec@(1,5) (40.3%, 84.0%)
07/01 04:42:45 PM | Train: [ 1/1] Step 781/781 Loss 1.657 Prec@(1,5) (40.9%, 84.4%)
07/01 04:42:45 PM | Train: [ 1/1] Final Prec@1 40.9440%
07/01 04:42:46 PM | Valid: [ 1/1] Step 000/781 Loss 1.234 Prec@(1,5) (65.6%, 90.6%)
07/01 04:43:39 PM | Valid: [ 1/1] Step 050/781 Loss 1.234 Prec@(1,5) (56.4%, 94.4%)
07/01 04:44:33 PM | Valid: [ 1/1] Step 100/781 Loss 1.208 Prec@(1,5) (57.4%, 94.5%)
07/01 04:45:25 PM | Valid: [ 1/1] Step 150/781 Loss 1.214 Prec@(1,5) (57.5%, 94.6%)
07/01 04:46:18 PM | Valid: [ 1/1] Step 200/781 Loss 1.212 Prec@(1,5) (57.7%, 94.7%)
07/01 04:47:11 PM | Valid: [ 1/1] Step 250/781 Loss 1.219 Prec@(1,5) (57.6%, 94.4%)
07/01 04:48:03 PM | Valid: [ 1/1] Step 300/781 Loss 1.221 Prec@(1,5) (57.6%, 94.3%)
07/01 04:48:55 PM | Valid: [ 1/1] Step 350/781 Loss 1.219 Prec@(1,5) (57.7%, 94.4%)
07/01 04:49:49 PM | Valid: [ 1/1] Step 400/781 Loss 1.215 Prec@(1,5) (57.9%, 94.5%)
07/01 04:50:41 PM | Valid: [ 1/1] Step 450/781 Loss 1.215 Prec@(1,5) (58.0%, 94.4%)
07/01 04:51:35 PM | Valid: [ 1/1] Step 500/781 Loss 1.212 Prec@(1,5) (57.9%, 94.5%)
07/01 04:52:27 PM | Valid: [ 1/1] Step 550/781 Loss 1.215 Prec@(1,5) (57.9%, 94.5%)
07/01 04:53:18 PM | Valid: [ 1/1] Step 600/781 Loss 1.213 Prec@(1,5) (58.0%, 94.5%)
07/01 04:54:09 PM | Valid: [ 1/1] Step 650/781 Loss 1.214 Prec@(1,5) (58.0%, 94.5%)
07/01 04:55:01 PM | Valid: [ 1/1] Step 700/781 Loss 1.214 Prec@(1,5) (58.1%, 94.4%)
07/01 04:55:52 PM | Valid: [ 1/1] Step 750/781 Loss 1.211 Prec@(1,5) (58.3%, 94.4%)
07/01 04:56:21 PM | Valid: [ 1/1] Step 781/781 Loss 1.211 Prec@(1,5) (58.3%, 94.4%)
07/01 04:56:21 PM | Valid: [ 1/1] Final Prec@1 58.2960%
07/01 04:56:21 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 0), ('dil_conv_3x3', 1)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('skip_connect', 2)]], normal_concat=range(2, 6), reduce=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 3)], [('skip_connect', 2), ('avg_pool_3x3', 1)]], reduce_concat=range(2, 6))
07/01 04:56:22 PM | Final best Prec@1 = 58.2960%
07/01 04:56:22 PM | Best Genotype = Genotype(normal=[[('avg_pool_3x3', 0), ('dil_conv_3x3', 1)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('skip_connect', 2)]], normal_concat=range(2, 6), reduce=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 3)], [('skip_connect', 2), ('avg_pool_3x3', 1)]], reduce_concat=range(2, 6))
07/02 12:08:40 AM | 
07/02 12:08:40 AM | Parameters:
07/02 12:08:40 AM | ALPHA_LR=0.0003
07/02 12:08:40 AM | ALPHA_WEIGHT_DECAY=0.001
07/02 12:08:40 AM | BATCH_SIZE=32
07/02 12:08:40 AM | DATA_PATH=./data/
07/02 12:08:40 AM | DATASET=cifar10
07/02 12:08:40 AM | EPOCHS=1
07/02 12:08:40 AM | GPUS=[0]
07/02 12:08:40 AM | INIT_CHANNELS=16
07/02 12:08:40 AM | LAYERS=8
07/02 12:08:40 AM | NAME=cifar10
07/02 12:08:40 AM | PATH=searchs/cifar10
07/02 12:08:40 AM | PLOT_PATH=searchs/cifar10/plots
07/02 12:08:40 AM | PRINT_FREQ=50
07/02 12:08:40 AM | SEED=2
07/02 12:08:40 AM | W_GRAD_CLIP=5.0
07/02 12:08:40 AM | W_LR=0.025
07/02 12:08:40 AM | W_LR_MIN=0.001
07/02 12:08:40 AM | W_MOMENTUM=0.9
07/02 12:08:40 AM | W_WEIGHT_DECAY=0.0003
07/02 12:08:40 AM | WORKERS=4
07/02 12:08:40 AM | 
07/02 12:08:40 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/02 12:08:53 AM | Train: [ 1/1] Step 000/781 Loss 2.623 Prec@(1,5) (9.4%, 34.4%)
07/02 12:13:54 AM | Train: [ 1/1] Step 050/781 Loss 2.418 Prec@(1,5) (13.2%, 56.1%)
07/02 12:18:58 AM | Train: [ 1/1] Step 100/781 Loss 2.339 Prec@(1,5) (15.3%, 60.0%)
07/02 12:23:57 AM | Train: [ 1/1] Step 150/781 Loss 2.257 Prec@(1,5) (18.2%, 64.3%)
07/02 12:28:54 AM | Train: [ 1/1] Step 200/781 Loss 2.182 Prec@(1,5) (20.9%, 68.1%)
07/02 12:33:53 AM | Train: [ 1/1] Step 250/781 Loss 2.113 Prec@(1,5) (23.3%, 71.2%)
07/02 12:38:53 AM | Train: [ 1/1] Step 300/781 Loss 2.045 Prec@(1,5) (26.0%, 73.7%)
07/02 12:43:46 AM | Train: [ 1/1] Step 350/781 Loss 1.980 Prec@(1,5) (28.4%, 76.1%)
07/02 12:48:49 AM | Train: [ 1/1] Step 400/781 Loss 1.926 Prec@(1,5) (30.4%, 77.8%)
07/02 12:53:50 AM | Train: [ 1/1] Step 450/781 Loss 1.878 Prec@(1,5) (32.4%, 79.4%)
07/02 12:58:53 AM | Train: [ 1/1] Step 500/781 Loss 1.826 Prec@(1,5) (34.3%, 80.9%)
07/02 01:03:58 AM | Train: [ 1/1] Step 550/781 Loss 1.781 Prec@(1,5) (36.0%, 82.0%)
07/02 01:09:00 AM | Train: [ 1/1] Step 600/781 Loss 1.743 Prec@(1,5) (37.4%, 83.0%)
07/02 01:13:54 AM | Train: [ 1/1] Step 650/781 Loss 1.707 Prec@(1,5) (38.7%, 83.8%)
07/02 01:18:56 AM | Train: [ 1/1] Step 700/781 Loss 1.674 Prec@(1,5) (40.0%, 84.6%)
07/02 01:23:55 AM | Train: [ 1/1] Step 750/781 Loss 1.640 Prec@(1,5) (41.3%, 85.2%)
07/02 01:26:57 AM | Train: [ 1/1] Step 781/781 Loss 1.622 Prec@(1,5) (42.0%, 85.6%)
07/02 01:26:57 AM | Train: [ 1/1] Final Prec@1 42.0000%
07/02 01:26:59 AM | Valid: [ 1/1] Step 000/781 Loss 1.001 Prec@(1,5) (71.9%, 100.0%)
07/02 01:27:51 AM | Valid: [ 1/1] Step 050/781 Loss 1.175 Prec@(1,5) (58.8%, 95.6%)
07/02 01:28:44 AM | Valid: [ 1/1] Step 100/781 Loss 1.161 Prec@(1,5) (58.8%, 95.7%)
07/02 01:29:35 AM | Valid: [ 1/1] Step 150/781 Loss 1.174 Prec@(1,5) (58.2%, 95.3%)
07/02 01:30:27 AM | Valid: [ 1/1] Step 200/781 Loss 1.173 Prec@(1,5) (58.3%, 95.3%)
07/02 01:31:17 AM | Valid: [ 1/1] Step 250/781 Loss 1.173 Prec@(1,5) (58.7%, 95.2%)
07/02 01:32:07 AM | Valid: [ 1/1] Step 300/781 Loss 1.171 Prec@(1,5) (58.8%, 95.2%)
07/02 01:32:58 AM | Valid: [ 1/1] Step 350/781 Loss 1.171 Prec@(1,5) (58.9%, 95.2%)
07/02 01:33:50 AM | Valid: [ 1/1] Step 400/781 Loss 1.163 Prec@(1,5) (59.2%, 95.3%)
07/02 01:34:41 AM | Valid: [ 1/1] Step 450/781 Loss 1.165 Prec@(1,5) (59.2%, 95.2%)
07/02 01:35:33 AM | Valid: [ 1/1] Step 500/781 Loss 1.162 Prec@(1,5) (59.3%, 95.3%)
07/02 01:36:26 AM | Valid: [ 1/1] Step 550/781 Loss 1.163 Prec@(1,5) (59.3%, 95.2%)
07/02 01:37:18 AM | Valid: [ 1/1] Step 600/781 Loss 1.163 Prec@(1,5) (59.3%, 95.3%)
07/02 01:38:10 AM | Valid: [ 1/1] Step 650/781 Loss 1.164 Prec@(1,5) (59.2%, 95.2%)
07/02 01:39:03 AM | Valid: [ 1/1] Step 700/781 Loss 1.166 Prec@(1,5) (59.2%, 95.2%)
07/02 01:39:54 AM | Valid: [ 1/1] Step 750/781 Loss 1.165 Prec@(1,5) (59.3%, 95.2%)
07/02 01:40:24 AM | Valid: [ 1/1] Step 781/781 Loss 1.165 Prec@(1,5) (59.3%, 95.2%)
07/02 01:40:24 AM | Valid: [ 1/1] Final Prec@1 59.3000%
07/02 01:40:24 AM | genotype = Genotype(normal=[[('avg_pool_3x3', 0), ('dil_conv_3x3', 1)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 1), ('skip_connect', 2)], [('avg_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 1)]], reduce_concat=range(2, 6))
07/02 08:54:55 AM | 
07/02 08:54:55 AM | Parameters:
07/02 08:54:55 AM | ALPHA_LR=0.0003
07/02 08:54:55 AM | ALPHA_WEIGHT_DECAY=0.001
07/02 08:54:55 AM | BATCH_SIZE=32
07/02 08:54:55 AM | DATA_PATH=./data/
07/02 08:54:55 AM | DATASET=cifar10
07/02 08:54:55 AM | EPOCHS=1
07/02 08:54:55 AM | GPUS=[0]
07/02 08:54:55 AM | INIT_CHANNELS=16
07/02 08:54:55 AM | LAYERS=8
07/02 08:54:55 AM | NAME=cifar10
07/02 08:54:55 AM | PATH=searchs/cifar10
07/02 08:54:55 AM | PLOT_PATH=searchs/cifar10/plots
07/02 08:54:55 AM | PRINT_FREQ=50
07/02 08:54:55 AM | SEED=2
07/02 08:54:55 AM | W_GRAD_CLIP=5.0
07/02 08:54:55 AM | W_LR=0.025
07/02 08:54:55 AM | W_LR_MIN=0.001
07/02 08:54:55 AM | W_MOMENTUM=0.9
07/02 08:54:55 AM | W_WEIGHT_DECAY=0.0003
07/02 08:54:55 AM | WORKERS=4
07/02 08:54:55 AM | 
07/02 08:54:55 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/02 08:55:09 AM | Train: [ 1/1] Step 000/781 Loss 2.623 Prec@(1,5) (9.4%, 34.4%)
07/02 09:00:13 AM | Train: [ 1/1] Step 050/781 Loss 2.417 Prec@(1,5) (13.4%, 56.9%)
07/02 09:05:15 AM | Train: [ 1/1] Step 100/781 Loss 2.351 Prec@(1,5) (15.3%, 59.6%)
07/02 09:10:16 AM | Train: [ 1/1] Step 150/781 Loss 2.269 Prec@(1,5) (18.2%, 63.6%)
07/02 09:15:25 AM | Train: [ 1/1] Step 200/781 Loss 2.194 Prec@(1,5) (20.7%, 67.3%)
07/02 09:20:28 AM | Train: [ 1/1] Step 250/781 Loss 2.129 Prec@(1,5) (23.2%, 70.5%)
07/02 09:25:34 AM | Train: [ 1/1] Step 300/781 Loss 2.063 Prec@(1,5) (25.6%, 73.1%)
07/02 09:28:41 AM | 
07/02 09:28:41 AM | Parameters:
07/02 09:28:41 AM | ALPHA_LR=0.0003
07/02 09:28:41 AM | ALPHA_WEIGHT_DECAY=0.001
07/02 09:28:41 AM | BATCH_SIZE=32
07/02 09:28:41 AM | DATA_PATH=./data/
07/02 09:28:41 AM | DATASET=cifar10
07/02 09:28:41 AM | EPOCHS=1
07/02 09:28:41 AM | GPUS=[0]
07/02 09:28:41 AM | INIT_CHANNELS=16
07/02 09:28:41 AM | LAYERS=8
07/02 09:28:41 AM | NAME=cifar10
07/02 09:28:41 AM | PATH=searchs/cifar10
07/02 09:28:41 AM | PLOT_PATH=searchs/cifar10/plots
07/02 09:28:41 AM | PRINT_FREQ=50
07/02 09:28:41 AM | SEED=2
07/02 09:28:41 AM | W_GRAD_CLIP=5.0
07/02 09:28:41 AM | W_LR=0.025
07/02 09:28:41 AM | W_LR_MIN=0.001
07/02 09:28:41 AM | W_MOMENTUM=0.9
07/02 09:28:41 AM | W_WEIGHT_DECAY=0.0003
07/02 09:28:41 AM | WORKERS=4
07/02 09:28:41 AM | 
07/02 09:28:41 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/02 09:28:53 AM | Train: [ 1/1] Step 000/781 Loss 2.623 Prec@(1,5) (9.4%, 34.4%)
07/02 09:33:55 AM | Train: [ 1/1] Step 050/781 Loss 2.429 Prec@(1,5) (13.3%, 55.9%)
07/02 09:38:53 AM | Train: [ 1/1] Step 100/781 Loss 2.345 Prec@(1,5) (15.5%, 59.7%)
07/02 09:43:51 AM | Train: [ 1/1] Step 150/781 Loss 2.267 Prec@(1,5) (18.6%, 63.6%)
07/02 09:48:41 AM | 
07/02 09:48:41 AM | Parameters:
07/02 09:48:41 AM | ALPHA_LR=0.0003
07/02 09:48:41 AM | ALPHA_WEIGHT_DECAY=0.001
07/02 09:48:41 AM | BATCH_SIZE=32
07/02 09:48:41 AM | DATA_PATH=./data/
07/02 09:48:41 AM | DATASET=cifar10
07/02 09:48:41 AM | EPOCHS=1
07/02 09:48:41 AM | GPUS=[0]
07/02 09:48:41 AM | INIT_CHANNELS=16
07/02 09:48:41 AM | LAYERS=8
07/02 09:48:41 AM | NAME=cifar10
07/02 09:48:41 AM | PATH=searchs/cifar10
07/02 09:48:41 AM | PLOT_PATH=searchs/cifar10/plots
07/02 09:48:41 AM | PRINT_FREQ=50
07/02 09:48:41 AM | SEED=2
07/02 09:48:41 AM | W_GRAD_CLIP=5.0
07/02 09:48:41 AM | W_LR=0.025
07/02 09:48:41 AM | W_LR_MIN=0.001
07/02 09:48:41 AM | W_MOMENTUM=0.9
07/02 09:48:41 AM | W_WEIGHT_DECAY=0.0003
07/02 09:48:41 AM | WORKERS=4
07/02 09:48:41 AM | 
07/02 09:48:41 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/02 09:48:54 AM | Train: [ 1/1] Step 200/781 Loss 2.192 Prec@(1,5) (21.5%, 67.4%)
07/02 09:48:54 AM | Train: [ 1/1] Step 000/781 Loss 2.623 Prec@(1,5) (9.4%, 34.4%)
07/02 09:53:56 AM | Train: [ 1/1] Step 050/781 Loss 2.441 Prec@(1,5) (12.7%, 54.4%)
07/02 09:53:56 AM | Train: [ 1/1] Step 250/781 Loss 2.129 Prec@(1,5) (23.5%, 70.6%)
07/02 09:58:58 AM | Train: [ 1/1] Step 300/781 Loss 2.066 Prec@(1,5) (25.7%, 73.3%)
07/02 09:58:58 AM | Train: [ 1/1] Step 100/781 Loss 2.353 Prec@(1,5) (14.6%, 59.1%)
07/02 10:03:59 AM | Train: [ 1/1] Step 150/781 Loss 2.284 Prec@(1,5) (17.0%, 62.5%)
07/02 10:04:02 AM | Train: [ 1/1] Step 350/781 Loss 2.004 Prec@(1,5) (28.0%, 75.4%)
07/02 10:09:02 AM | Train: [ 1/1] Step 200/781 Loss 2.203 Prec@(1,5) (20.0%, 66.7%)
07/02 10:09:08 AM | Train: [ 1/1] Step 400/781 Loss 1.948 Prec@(1,5) (30.0%, 77.2%)
07/02 10:14:05 AM | Train: [ 1/1] Step 250/781 Loss 2.133 Prec@(1,5) (22.6%, 70.2%)
07/02 10:14:12 AM | Train: [ 1/1] Step 450/781 Loss 1.899 Prec@(1,5) (31.7%, 78.8%)
07/02 10:19:06 AM | Train: [ 1/1] Step 300/781 Loss 2.068 Prec@(1,5) (25.1%, 72.9%)
07/02 10:19:18 AM | Train: [ 1/1] Step 500/781 Loss 1.847 Prec@(1,5) (33.5%, 80.3%)
07/02 10:24:07 AM | Train: [ 1/1] Step 350/781 Loss 2.002 Prec@(1,5) (27.7%, 75.2%)
07/02 10:24:19 AM | Train: [ 1/1] Step 550/781 Loss 1.805 Prec@(1,5) (35.1%, 81.4%)
07/02 10:29:08 AM | Train: [ 1/1] Step 400/781 Loss 1.943 Prec@(1,5) (29.9%, 77.2%)
07/02 10:29:23 AM | Train: [ 1/1] Step 600/781 Loss 1.767 Prec@(1,5) (36.5%, 82.4%)
07/02 10:34:09 AM | Train: [ 1/1] Step 450/781 Loss 1.894 Prec@(1,5) (31.7%, 78.7%)
07/02 10:34:27 AM | Train: [ 1/1] Step 650/781 Loss 1.735 Prec@(1,5) (37.8%, 83.3%)
07/02 10:39:06 AM | Train: [ 1/1] Step 500/781 Loss 1.843 Prec@(1,5) (33.4%, 80.1%)
07/02 10:39:32 AM | Train: [ 1/1] Step 700/781 Loss 1.701 Prec@(1,5) (39.1%, 84.1%)
07/02 10:44:07 AM | Train: [ 1/1] Step 550/781 Loss 1.798 Prec@(1,5) (35.0%, 81.3%)
07/02 10:44:38 AM | Train: [ 1/1] Step 750/781 Loss 1.669 Prec@(1,5) (40.2%, 84.8%)
07/02 10:47:45 AM | Train: [ 1/1] Step 781/781 Loss 1.651 Prec@(1,5) (40.9%, 85.2%)
07/02 10:47:45 AM | Train: [ 1/1] Final Prec@1 40.9240%
07/02 10:47:47 AM | Valid: [ 1/1] Step 000/781 Loss 0.956 Prec@(1,5) (62.5%, 100.0%)
07/02 10:48:38 AM | Valid: [ 1/1] Step 050/781 Loss 1.193 Prec@(1,5) (58.0%, 95.3%)
07/02 10:49:07 AM | Train: [ 1/1] Step 600/781 Loss 1.760 Prec@(1,5) (36.6%, 82.2%)
07/02 10:49:29 AM | Valid: [ 1/1] Step 100/781 Loss 1.174 Prec@(1,5) (58.8%, 95.2%)
07/02 10:50:20 AM | Valid: [ 1/1] Step 150/781 Loss 1.188 Prec@(1,5) (58.3%, 95.2%)
07/02 10:51:12 AM | Valid: [ 1/1] Step 200/781 Loss 1.184 Prec@(1,5) (58.6%, 95.2%)
07/02 10:52:03 AM | Valid: [ 1/1] Step 250/781 Loss 1.183 Prec@(1,5) (58.4%, 95.0%)
07/02 10:52:55 AM | Valid: [ 1/1] Step 300/781 Loss 1.187 Prec@(1,5) (58.2%, 95.0%)
07/02 10:53:47 AM | Valid: [ 1/1] Step 350/781 Loss 1.184 Prec@(1,5) (58.4%, 95.0%)
07/02 10:54:10 AM | Train: [ 1/1] Step 650/781 Loss 1.725 Prec@(1,5) (38.0%, 83.1%)
07/02 10:54:40 AM | Valid: [ 1/1] Step 400/781 Loss 1.177 Prec@(1,5) (58.9%, 95.2%)
07/02 10:55:33 AM | Valid: [ 1/1] Step 450/781 Loss 1.176 Prec@(1,5) (59.0%, 95.2%)
07/02 10:56:25 AM | Valid: [ 1/1] Step 500/781 Loss 1.174 Prec@(1,5) (59.0%, 95.2%)
07/02 10:57:18 AM | Valid: [ 1/1] Step 550/781 Loss 1.175 Prec@(1,5) (59.1%, 95.0%)
07/02 10:58:10 AM | Valid: [ 1/1] Step 600/781 Loss 1.173 Prec@(1,5) (59.1%, 95.1%)
07/02 10:59:03 AM | Valid: [ 1/1] Step 650/781 Loss 1.174 Prec@(1,5) (59.0%, 95.1%)
07/02 10:59:12 AM | Train: [ 1/1] Step 700/781 Loss 1.692 Prec@(1,5) (39.3%, 83.9%)
07/02 10:59:55 AM | Valid: [ 1/1] Step 700/781 Loss 1.173 Prec@(1,5) (59.1%, 95.1%)
07/02 11:00:48 AM | Valid: [ 1/1] Step 750/781 Loss 1.170 Prec@(1,5) (59.2%, 95.1%)
07/02 11:01:18 AM | Valid: [ 1/1] Step 781/781 Loss 1.170 Prec@(1,5) (59.2%, 95.1%)
07/02 11:01:19 AM | Valid: [ 1/1] Final Prec@1 59.1880%
07/02 11:01:19 AM | genotype = Genotype(normal=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 4)]], normal_concat=range(2, 6), reduce=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 0), ('skip_connect', 2)]], reduce_concat=range(2, 6))
07/02 11:04:12 AM | Train: [ 1/1] Step 750/781 Loss 1.658 Prec@(1,5) (40.5%, 84.6%)
07/02 11:07:15 AM | Train: [ 1/1] Step 781/781 Loss 1.642 Prec@(1,5) (41.2%, 85.0%)
07/02 11:07:15 AM | Train: [ 1/1] Final Prec@1 41.1920%
07/02 11:07:17 AM | Valid: [ 1/1] Step 000/781 Loss 1.181 Prec@(1,5) (62.5%, 96.9%)
07/02 11:08:08 AM | Valid: [ 1/1] Step 050/781 Loss 1.218 Prec@(1,5) (57.0%, 94.3%)
07/02 11:08:59 AM | Valid: [ 1/1] Step 100/781 Loss 1.186 Prec@(1,5) (58.7%, 95.0%)
07/02 11:09:51 AM | Valid: [ 1/1] Step 150/781 Loss 1.191 Prec@(1,5) (58.4%, 95.0%)
07/02 11:10:44 AM | Valid: [ 1/1] Step 200/781 Loss 1.186 Prec@(1,5) (58.8%, 95.1%)
07/02 11:11:38 AM | Valid: [ 1/1] Step 250/781 Loss 1.185 Prec@(1,5) (58.7%, 95.1%)
07/02 11:12:32 AM | Valid: [ 1/1] Step 300/781 Loss 1.191 Prec@(1,5) (58.6%, 94.8%)
07/02 11:13:24 AM | Valid: [ 1/1] Step 350/781 Loss 1.186 Prec@(1,5) (58.9%, 95.0%)
07/02 11:14:17 AM | Valid: [ 1/1] Step 400/781 Loss 1.180 Prec@(1,5) (59.0%, 95.2%)
07/02 11:15:10 AM | Valid: [ 1/1] Step 450/781 Loss 1.178 Prec@(1,5) (59.2%, 95.2%)
07/02 11:16:03 AM | Valid: [ 1/1] Step 500/781 Loss 1.176 Prec@(1,5) (59.2%, 95.2%)
07/02 11:16:56 AM | Valid: [ 1/1] Step 550/781 Loss 1.176 Prec@(1,5) (59.5%, 95.1%)
07/02 11:17:47 AM | Valid: [ 1/1] Step 600/781 Loss 1.175 Prec@(1,5) (59.5%, 95.1%)
07/02 11:18:39 AM | Valid: [ 1/1] Step 650/781 Loss 1.174 Prec@(1,5) (59.5%, 95.1%)
07/02 11:19:30 AM | Valid: [ 1/1] Step 700/781 Loss 1.174 Prec@(1,5) (59.5%, 95.0%)
07/02 11:20:23 AM | Valid: [ 1/1] Step 750/781 Loss 1.175 Prec@(1,5) (59.5%, 95.0%)
07/02 11:20:54 AM | Valid: [ 1/1] Step 781/781 Loss 1.176 Prec@(1,5) (59.4%, 95.0%)
07/02 11:20:55 AM | Valid: [ 1/1] Final Prec@1 59.4040%
07/02 11:20:55 AM | genotype = Genotype(normal=[[('avg_pool_3x3', 0), ('dil_conv_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 4)]], normal_concat=range(2, 6), reduce=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 4)]], reduce_concat=range(2, 6))
07/02 11:20:55 AM | Final best Prec@1 = 59.4040%
07/02 11:20:55 AM | Best Genotype = Genotype(normal=[[('avg_pool_3x3', 0), ('dil_conv_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 0), ('skip_connect', 4)]], normal_concat=range(2, 6), reduce=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 4)]], reduce_concat=range(2, 6))
07/02 03:06:48 PM | 
07/02 03:06:48 PM | Parameters:
07/02 03:06:48 PM | ALPHA_LR=0.0003
07/02 03:06:48 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 03:06:48 PM | BATCH_SIZE=32
07/02 03:06:48 PM | DATA_PATH=./data/
07/02 03:06:48 PM | DATASET=cifar10
07/02 03:06:48 PM | EPOCHS=1
07/02 03:06:48 PM | GPUS=[0]
07/02 03:06:48 PM | INIT_CHANNELS=16
07/02 03:06:48 PM | LAYERS=8
07/02 03:06:48 PM | NAME=cifar10
07/02 03:06:48 PM | PATH=searchs/cifar10
07/02 03:06:48 PM | PLOT_PATH=searchs/cifar10/plots
07/02 03:06:48 PM | PRINT_FREQ=50
07/02 03:06:48 PM | SEED=2
07/02 03:06:48 PM | W_GRAD_CLIP=5.0
07/02 03:06:48 PM | W_LR=0.025
07/02 03:06:48 PM | W_LR_MIN=0.001
07/02 03:06:48 PM | W_MOMENTUM=0.9
07/02 03:06:48 PM | W_WEIGHT_DECAY=0.0003
07/02 03:06:48 PM | WORKERS=4
07/02 03:06:48 PM | 
07/02 03:06:48 PM | Logger is set - training start
07/02 04:44:19 PM | 
07/02 04:44:19 PM | Parameters:
07/02 04:44:19 PM | ALPHA_LR=0.0003
07/02 04:44:19 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 04:44:19 PM | BATCH_SIZE=32
07/02 04:44:19 PM | DATA_PATH=./data/
07/02 04:44:19 PM | DATASET=cifar10
07/02 04:44:19 PM | EPOCHS=1
07/02 04:44:19 PM | GPUS=[0]
07/02 04:44:19 PM | INIT_CHANNELS=16
07/02 04:44:19 PM | LAYERS=8
07/02 04:44:19 PM | NAME=cifar10
07/02 04:44:19 PM | PATH=searchs/cifar10
07/02 04:44:19 PM | PLOT_PATH=searchs/cifar10/plots
07/02 04:44:19 PM | PRINT_FREQ=50
07/02 04:44:19 PM | SEED=2
07/02 04:44:19 PM | W_GRAD_CLIP=5.0
07/02 04:44:19 PM | W_LR=0.025
07/02 04:44:19 PM | W_LR_MIN=0.001
07/02 04:44:19 PM | W_MOMENTUM=0.9
07/02 04:44:19 PM | W_WEIGHT_DECAY=0.0003
07/02 04:44:19 PM | WORKERS=4
07/02 04:44:19 PM | 
07/02 04:44:19 PM | Logger is set - training start
07/02 04:47:45 PM | 
07/02 04:47:45 PM | Parameters:
07/02 04:47:45 PM | ALPHA_LR=0.0003
07/02 04:47:45 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 04:47:45 PM | BATCH_SIZE=32
07/02 04:47:45 PM | DATA_PATH=./data/
07/02 04:47:45 PM | DATASET=cifar10
07/02 04:47:45 PM | EPOCHS=1
07/02 04:47:45 PM | GPUS=[0]
07/02 04:47:45 PM | INIT_CHANNELS=16
07/02 04:47:45 PM | LAYERS=8
07/02 04:47:45 PM | NAME=cifar10
07/02 04:47:45 PM | PATH=searchs/cifar10
07/02 04:47:45 PM | PLOT_PATH=searchs/cifar10/plots
07/02 04:47:45 PM | PRINT_FREQ=50
07/02 04:47:45 PM | SEED=2
07/02 04:47:45 PM | W_GRAD_CLIP=5.0
07/02 04:47:45 PM | W_LR=0.025
07/02 04:47:45 PM | W_LR_MIN=0.001
07/02 04:47:45 PM | W_MOMENTUM=0.9
07/02 04:47:45 PM | W_WEIGHT_DECAY=0.0003
07/02 04:47:45 PM | WORKERS=4
07/02 04:47:45 PM | 
07/02 04:47:45 PM | Logger is set - training start
07/02 04:48:46 PM | 
07/02 04:48:46 PM | Parameters:
07/02 04:48:46 PM | ALPHA_LR=0.0003
07/02 04:48:46 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 04:48:46 PM | BATCH_SIZE=32
07/02 04:48:46 PM | DATA_PATH=./data/
07/02 04:48:46 PM | DATASET=cifar10
07/02 04:48:46 PM | EPOCHS=1
07/02 04:48:46 PM | GPUS=[0]
07/02 04:48:46 PM | INIT_CHANNELS=16
07/02 04:48:46 PM | LAYERS=8
07/02 04:48:46 PM | NAME=cifar10
07/02 04:48:46 PM | PATH=searchs/cifar10
07/02 04:48:46 PM | PLOT_PATH=searchs/cifar10/plots
07/02 04:48:46 PM | PRINT_FREQ=50
07/02 04:48:46 PM | SEED=2
07/02 04:48:46 PM | W_GRAD_CLIP=5.0
07/02 04:48:46 PM | W_LR=0.025
07/02 04:48:46 PM | W_LR_MIN=0.001
07/02 04:48:46 PM | W_MOMENTUM=0.9
07/02 04:48:46 PM | W_WEIGHT_DECAY=0.0003
07/02 04:48:46 PM | WORKERS=4
07/02 04:48:46 PM | 
07/02 04:48:46 PM | Logger is set - training start
07/02 04:54:01 PM | 
07/02 04:54:01 PM | Parameters:
07/02 04:54:01 PM | ALPHA_LR=0.0003
07/02 04:54:01 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 04:54:01 PM | BATCH_SIZE=32
07/02 04:54:01 PM | DATA_PATH=./data/
07/02 04:54:01 PM | DATASET=cifar10
07/02 04:54:01 PM | EPOCHS=1
07/02 04:54:01 PM | GPUS=[0]
07/02 04:54:01 PM | INIT_CHANNELS=16
07/02 04:54:01 PM | LAYERS=8
07/02 04:54:01 PM | NAME=cifar10
07/02 04:54:01 PM | PATH=searchs/cifar10
07/02 04:54:01 PM | PLOT_PATH=searchs/cifar10/plots
07/02 04:54:01 PM | PRINT_FREQ=50
07/02 04:54:01 PM | SEED=2
07/02 04:54:01 PM | W_GRAD_CLIP=5.0
07/02 04:54:01 PM | W_LR=0.025
07/02 04:54:01 PM | W_LR_MIN=0.001
07/02 04:54:01 PM | W_MOMENTUM=0.9
07/02 04:54:01 PM | W_WEIGHT_DECAY=0.0003
07/02 04:54:01 PM | WORKERS=4
07/02 04:54:01 PM | 
07/02 04:54:01 PM | Logger is set - training start
07/02 04:54:28 PM | 
07/02 04:54:28 PM | Parameters:
07/02 04:54:28 PM | ALPHA_LR=0.0003
07/02 04:54:28 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 04:54:28 PM | BATCH_SIZE=32
07/02 04:54:28 PM | DATA_PATH=./data/
07/02 04:54:28 PM | DATASET=cifar10
07/02 04:54:28 PM | EPOCHS=1
07/02 04:54:28 PM | GPUS=[0]
07/02 04:54:28 PM | INIT_CHANNELS=16
07/02 04:54:28 PM | LAYERS=8
07/02 04:54:28 PM | NAME=cifar10
07/02 04:54:28 PM | PATH=searchs/cifar10
07/02 04:54:28 PM | PLOT_PATH=searchs/cifar10/plots
07/02 04:54:28 PM | PRINT_FREQ=50
07/02 04:54:28 PM | SEED=2
07/02 04:54:28 PM | W_GRAD_CLIP=5.0
07/02 04:54:28 PM | W_LR=0.025
07/02 04:54:28 PM | W_LR_MIN=0.001
07/02 04:54:28 PM | W_MOMENTUM=0.9
07/02 04:54:28 PM | W_WEIGHT_DECAY=0.0003
07/02 04:54:28 PM | WORKERS=4
07/02 04:54:28 PM | 
07/02 04:54:28 PM | Logger is set - training start
07/02 04:54:42 PM | 
07/02 04:54:42 PM | Parameters:
07/02 04:54:42 PM | ALPHA_LR=0.0003
07/02 04:54:42 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 04:54:42 PM | BATCH_SIZE=32
07/02 04:54:42 PM | DATA_PATH=./data/
07/02 04:54:42 PM | DATASET=cifar10
07/02 04:54:42 PM | EPOCHS=1
07/02 04:54:42 PM | GPUS=[0]
07/02 04:54:42 PM | INIT_CHANNELS=16
07/02 04:54:42 PM | LAYERS=8
07/02 04:54:42 PM | NAME=cifar10
07/02 04:54:42 PM | PATH=searchs/cifar10
07/02 04:54:42 PM | PLOT_PATH=searchs/cifar10/plots
07/02 04:54:42 PM | PRINT_FREQ=50
07/02 04:54:42 PM | SEED=2
07/02 04:54:42 PM | W_GRAD_CLIP=5.0
07/02 04:54:42 PM | W_LR=0.025
07/02 04:54:42 PM | W_LR_MIN=0.001
07/02 04:54:42 PM | W_MOMENTUM=0.9
07/02 04:54:42 PM | W_WEIGHT_DECAY=0.0003
07/02 04:54:42 PM | WORKERS=4
07/02 04:54:42 PM | 
07/02 04:54:42 PM | Logger is set - training start
07/02 05:08:43 PM | 
07/02 05:08:43 PM | Parameters:
07/02 05:08:43 PM | ALPHA_LR=0.0003
07/02 05:08:43 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 05:08:43 PM | BATCH_SIZE=32
07/02 05:08:43 PM | DATA_PATH=./data/
07/02 05:08:43 PM | DATASET=cifar10
07/02 05:08:43 PM | EPOCHS=1
07/02 05:08:43 PM | GPUS=[0]
07/02 05:08:43 PM | INIT_CHANNELS=16
07/02 05:08:43 PM | LAYERS=8
07/02 05:08:43 PM | NAME=cifar10
07/02 05:08:43 PM | PATH=searchs/cifar10
07/02 05:08:43 PM | PLOT_PATH=searchs/cifar10/plots
07/02 05:08:43 PM | PRINT_FREQ=50
07/02 05:08:43 PM | SEED=2
07/02 05:08:43 PM | W_GRAD_CLIP=5.0
07/02 05:08:43 PM | W_LR=0.025
07/02 05:08:43 PM | W_LR_MIN=0.001
07/02 05:08:43 PM | W_MOMENTUM=0.9
07/02 05:08:43 PM | W_WEIGHT_DECAY=0.0003
07/02 05:08:43 PM | WORKERS=4
07/02 05:08:43 PM | 
07/02 05:08:43 PM | Logger is set - training start
07/02 05:08:58 PM | 
07/02 05:08:58 PM | Parameters:
07/02 05:08:58 PM | ALPHA_LR=0.0003
07/02 05:08:58 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 05:08:58 PM | BATCH_SIZE=32
07/02 05:08:58 PM | DATA_PATH=./data/
07/02 05:08:58 PM | DATASET=cifar10
07/02 05:08:58 PM | EPOCHS=1
07/02 05:08:58 PM | GPUS=[0]
07/02 05:08:58 PM | INIT_CHANNELS=16
07/02 05:08:58 PM | LAYERS=8
07/02 05:08:58 PM | NAME=cifar10
07/02 05:08:58 PM | PATH=searchs/cifar10
07/02 05:08:58 PM | PLOT_PATH=searchs/cifar10/plots
07/02 05:08:58 PM | PRINT_FREQ=50
07/02 05:08:58 PM | SEED=2
07/02 05:08:58 PM | W_GRAD_CLIP=5.0
07/02 05:08:58 PM | W_LR=0.025
07/02 05:08:58 PM | W_LR_MIN=0.001
07/02 05:08:58 PM | W_MOMENTUM=0.9
07/02 05:08:58 PM | W_WEIGHT_DECAY=0.0003
07/02 05:08:58 PM | WORKERS=4
07/02 05:08:58 PM | 
07/02 05:08:58 PM | Logger is set - training start
07/02 05:09:18 PM | 
07/02 05:09:18 PM | Parameters:
07/02 05:09:18 PM | ALPHA_LR=0.0003
07/02 05:09:18 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 05:09:18 PM | BATCH_SIZE=32
07/02 05:09:18 PM | DATA_PATH=./data/
07/02 05:09:18 PM | DATASET=cifar10
07/02 05:09:18 PM | EPOCHS=1
07/02 05:09:18 PM | GPUS=[0]
07/02 05:09:18 PM | INIT_CHANNELS=16
07/02 05:09:18 PM | LAYERS=8
07/02 05:09:18 PM | NAME=cifar10
07/02 05:09:18 PM | PATH=searchs/cifar10
07/02 05:09:18 PM | PLOT_PATH=searchs/cifar10/plots
07/02 05:09:18 PM | PRINT_FREQ=50
07/02 05:09:18 PM | SEED=2
07/02 05:09:18 PM | W_GRAD_CLIP=5.0
07/02 05:09:18 PM | W_LR=0.025
07/02 05:09:18 PM | W_LR_MIN=0.001
07/02 05:09:18 PM | W_MOMENTUM=0.9
07/02 05:09:18 PM | W_WEIGHT_DECAY=0.0003
07/02 05:09:18 PM | WORKERS=4
07/02 05:09:18 PM | 
07/02 05:09:18 PM | Logger is set - training start
07/02 05:10:16 PM | 
07/02 05:10:16 PM | Parameters:
07/02 05:10:16 PM | ALPHA_LR=0.0003
07/02 05:10:16 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 05:10:16 PM | BATCH_SIZE=32
07/02 05:10:16 PM | DATA_PATH=./data/
07/02 05:10:16 PM | DATASET=cifar10
07/02 05:10:16 PM | EPOCHS=1
07/02 05:10:16 PM | GPUS=[0]
07/02 05:10:16 PM | INIT_CHANNELS=16
07/02 05:10:16 PM | LAYERS=8
07/02 05:10:16 PM | NAME=cifar10
07/02 05:10:16 PM | PATH=searchs/cifar10
07/02 05:10:16 PM | PLOT_PATH=searchs/cifar10/plots
07/02 05:10:16 PM | PRINT_FREQ=50
07/02 05:10:16 PM | SEED=2
07/02 05:10:16 PM | W_GRAD_CLIP=5.0
07/02 05:10:16 PM | W_LR=0.025
07/02 05:10:16 PM | W_LR_MIN=0.001
07/02 05:10:16 PM | W_MOMENTUM=0.9
07/02 05:10:16 PM | W_WEIGHT_DECAY=0.0003
07/02 05:10:16 PM | WORKERS=4
07/02 05:10:16 PM | 
07/02 05:10:16 PM | Logger is set - training start
07/02 05:10:27 PM | 
07/02 05:10:27 PM | Parameters:
07/02 05:10:27 PM | ALPHA_LR=0.0003
07/02 05:10:27 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 05:10:27 PM | BATCH_SIZE=32
07/02 05:10:27 PM | DATA_PATH=./data/
07/02 05:10:27 PM | DATASET=cifar10
07/02 05:10:27 PM | EPOCHS=1
07/02 05:10:27 PM | GPUS=[0]
07/02 05:10:27 PM | INIT_CHANNELS=16
07/02 05:10:27 PM | LAYERS=8
07/02 05:10:27 PM | NAME=cifar10
07/02 05:10:27 PM | PATH=searchs/cifar10
07/02 05:10:27 PM | PLOT_PATH=searchs/cifar10/plots
07/02 05:10:27 PM | PRINT_FREQ=50
07/02 05:10:27 PM | SEED=2
07/02 05:10:27 PM | W_GRAD_CLIP=5.0
07/02 05:10:27 PM | W_LR=0.025
07/02 05:10:27 PM | W_LR_MIN=0.001
07/02 05:10:27 PM | W_MOMENTUM=0.9
07/02 05:10:27 PM | W_WEIGHT_DECAY=0.0003
07/02 05:10:27 PM | WORKERS=4
07/02 05:10:27 PM | 
07/02 05:10:27 PM | Logger is set - training start
07/02 09:23:46 PM | 
07/02 09:23:46 PM | Parameters:
07/02 09:23:46 PM | ALPHA_LR=0.0003
07/02 09:23:46 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 09:23:46 PM | BATCH_SIZE=32
07/02 09:23:46 PM | DATA_PATH=./data/
07/02 09:23:46 PM | DATASET=cifar10
07/02 09:23:46 PM | EPOCHS=1
07/02 09:23:46 PM | GPUS=[0]
07/02 09:23:46 PM | INIT_CHANNELS=16
07/02 09:23:46 PM | LAYERS=8
07/02 09:23:46 PM | NAME=cifar10
07/02 09:23:46 PM | PATH=searchs/cifar10
07/02 09:23:46 PM | PLOT_PATH=searchs/cifar10/plots
07/02 09:23:46 PM | PRINT_FREQ=50
07/02 09:23:46 PM | SEED=2
07/02 09:23:46 PM | W_GRAD_CLIP=5.0
07/02 09:23:46 PM | W_LR=0.025
07/02 09:23:46 PM | W_LR_MIN=0.001
07/02 09:23:46 PM | W_MOMENTUM=0.9
07/02 09:23:46 PM | W_WEIGHT_DECAY=0.0003
07/02 09:23:46 PM | WORKERS=4
07/02 09:23:46 PM | 
07/02 09:23:46 PM | Logger is set - training start
07/02 09:25:32 PM | 
07/02 09:25:32 PM | Parameters:
07/02 09:25:32 PM | ALPHA_LR=0.0003
07/02 09:25:32 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 09:25:32 PM | BATCH_SIZE=32
07/02 09:25:32 PM | DATA_PATH=./data/
07/02 09:25:32 PM | DATASET=cifar10
07/02 09:25:32 PM | EPOCHS=1
07/02 09:25:32 PM | GPUS=[0]
07/02 09:25:32 PM | INIT_CHANNELS=16
07/02 09:25:32 PM | LAYERS=8
07/02 09:25:32 PM | NAME=cifar10
07/02 09:25:32 PM | PATH=searchs/cifar10
07/02 09:25:32 PM | PLOT_PATH=searchs/cifar10/plots
07/02 09:25:32 PM | PRINT_FREQ=50
07/02 09:25:32 PM | SEED=2
07/02 09:25:32 PM | W_GRAD_CLIP=5.0
07/02 09:25:32 PM | W_LR=0.025
07/02 09:25:32 PM | W_LR_MIN=0.001
07/02 09:25:32 PM | W_MOMENTUM=0.9
07/02 09:25:32 PM | W_WEIGHT_DECAY=0.0003
07/02 09:25:32 PM | WORKERS=4
07/02 09:25:32 PM | 
07/02 09:25:32 PM | Logger is set - training start
07/02 09:27:23 PM | 
07/02 09:27:23 PM | Parameters:
07/02 09:27:23 PM | ALPHA_LR=0.0003
07/02 09:27:23 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 09:27:23 PM | BATCH_SIZE=32
07/02 09:27:23 PM | DATA_PATH=./data/
07/02 09:27:23 PM | DATASET=cifar10
07/02 09:27:23 PM | EPOCHS=1
07/02 09:27:23 PM | GPUS=[0]
07/02 09:27:23 PM | INIT_CHANNELS=16
07/02 09:27:23 PM | LAYERS=8
07/02 09:27:23 PM | NAME=cifar10
07/02 09:27:23 PM | PATH=searchs/cifar10
07/02 09:27:23 PM | PLOT_PATH=searchs/cifar10/plots
07/02 09:27:23 PM | PRINT_FREQ=50
07/02 09:27:23 PM | SEED=2
07/02 09:27:23 PM | W_GRAD_CLIP=5.0
07/02 09:27:23 PM | W_LR=0.025
07/02 09:27:23 PM | W_LR_MIN=0.001
07/02 09:27:23 PM | W_MOMENTUM=0.9
07/02 09:27:23 PM | W_WEIGHT_DECAY=0.0003
07/02 09:27:23 PM | WORKERS=4
07/02 09:27:23 PM | 
07/02 09:27:23 PM | Logger is set - training start
07/02 11:05:06 PM | 
07/02 11:05:06 PM | Parameters:
07/02 11:05:06 PM | ALPHA_LR=0.0003
07/02 11:05:06 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 11:05:06 PM | BATCH_SIZE=32
07/02 11:05:06 PM | DATA_PATH=./data/
07/02 11:05:06 PM | DATASET=cifar10
07/02 11:05:06 PM | EPOCHS=1
07/02 11:05:06 PM | GPUS=[0]
07/02 11:05:06 PM | INIT_CHANNELS=16
07/02 11:05:06 PM | LAYERS=8
07/02 11:05:06 PM | NAME=cifar10
07/02 11:05:06 PM | PATH=searchs/cifar10
07/02 11:05:06 PM | PLOT_PATH=searchs/cifar10/plots
07/02 11:05:06 PM | PRINT_FREQ=50
07/02 11:05:06 PM | SEED=2
07/02 11:05:06 PM | W_GRAD_CLIP=5.0
07/02 11:05:06 PM | W_LR=0.025
07/02 11:05:06 PM | W_LR_MIN=0.001
07/02 11:05:06 PM | W_MOMENTUM=0.9
07/02 11:05:06 PM | W_WEIGHT_DECAY=0.0003
07/02 11:05:06 PM | WORKERS=4
07/02 11:05:06 PM | 
07/02 11:05:06 PM | Logger is set - training start
07/02 11:06:20 PM | 
07/02 11:06:20 PM | Parameters:
07/02 11:06:20 PM | ALPHA_LR=0.0003
07/02 11:06:20 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 11:06:20 PM | BATCH_SIZE=32
07/02 11:06:20 PM | DATA_PATH=./data/
07/02 11:06:20 PM | DATASET=cifar10
07/02 11:06:20 PM | EPOCHS=1
07/02 11:06:20 PM | GPUS=[0]
07/02 11:06:20 PM | INIT_CHANNELS=16
07/02 11:06:20 PM | LAYERS=8
07/02 11:06:20 PM | NAME=cifar10
07/02 11:06:20 PM | PATH=searchs/cifar10
07/02 11:06:20 PM | PLOT_PATH=searchs/cifar10/plots
07/02 11:06:20 PM | PRINT_FREQ=50
07/02 11:06:20 PM | SEED=2
07/02 11:06:20 PM | W_GRAD_CLIP=5.0
07/02 11:06:20 PM | W_LR=0.025
07/02 11:06:20 PM | W_LR_MIN=0.001
07/02 11:06:20 PM | W_MOMENTUM=0.9
07/02 11:06:20 PM | W_WEIGHT_DECAY=0.0003
07/02 11:06:20 PM | WORKERS=4
07/02 11:06:20 PM | 
07/02 11:06:20 PM | Logger is set - training start
07/02 11:11:17 PM | 
07/02 11:11:17 PM | Parameters:
07/02 11:11:17 PM | ALPHA_LR=0.0003
07/02 11:11:17 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 11:11:17 PM | BATCH_SIZE=32
07/02 11:11:17 PM | DATA_PATH=./data/
07/02 11:11:17 PM | DATASET=cifar10
07/02 11:11:17 PM | EPOCHS=1
07/02 11:11:17 PM | GPUS=[0]
07/02 11:11:17 PM | INIT_CHANNELS=16
07/02 11:11:17 PM | LAYERS=8
07/02 11:11:17 PM | NAME=cifar10
07/02 11:11:17 PM | PATH=searchs/cifar10
07/02 11:11:17 PM | PLOT_PATH=searchs/cifar10/plots
07/02 11:11:17 PM | PRINT_FREQ=50
07/02 11:11:17 PM | SEED=2
07/02 11:11:17 PM | W_GRAD_CLIP=5.0
07/02 11:11:17 PM | W_LR=0.025
07/02 11:11:17 PM | W_LR_MIN=0.001
07/02 11:11:17 PM | W_MOMENTUM=0.9
07/02 11:11:17 PM | W_WEIGHT_DECAY=0.0003
07/02 11:11:17 PM | WORKERS=4
07/02 11:11:17 PM | 
07/02 11:11:17 PM | Logger is set - training start
07/02 11:13:17 PM | 
07/02 11:13:17 PM | Parameters:
07/02 11:13:17 PM | ALPHA_LR=0.0003
07/02 11:13:17 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 11:13:17 PM | BATCH_SIZE=32
07/02 11:13:17 PM | DATA_PATH=./data/
07/02 11:13:17 PM | DATASET=cifar10
07/02 11:13:17 PM | EPOCHS=1
07/02 11:13:17 PM | GPUS=[0]
07/02 11:13:17 PM | INIT_CHANNELS=16
07/02 11:13:17 PM | LAYERS=8
07/02 11:13:17 PM | NAME=cifar10
07/02 11:13:17 PM | PATH=searchs/cifar10
07/02 11:13:17 PM | PLOT_PATH=searchs/cifar10/plots
07/02 11:13:17 PM | PRINT_FREQ=50
07/02 11:13:17 PM | SEED=2
07/02 11:13:17 PM | W_GRAD_CLIP=5.0
07/02 11:13:17 PM | W_LR=0.025
07/02 11:13:17 PM | W_LR_MIN=0.001
07/02 11:13:17 PM | W_MOMENTUM=0.9
07/02 11:13:17 PM | W_WEIGHT_DECAY=0.0003
07/02 11:13:17 PM | WORKERS=4
07/02 11:13:17 PM | 
07/02 11:13:17 PM | Logger is set - training start
07/02 11:14:27 PM | 
07/02 11:14:27 PM | Parameters:
07/02 11:14:27 PM | ALPHA_LR=0.0003
07/02 11:14:27 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 11:14:27 PM | BATCH_SIZE=32
07/02 11:14:27 PM | DATA_PATH=./data/
07/02 11:14:27 PM | DATASET=cifar10
07/02 11:14:27 PM | EPOCHS=1
07/02 11:14:27 PM | GPUS=[0]
07/02 11:14:27 PM | INIT_CHANNELS=16
07/02 11:14:27 PM | LAYERS=8
07/02 11:14:27 PM | NAME=cifar10
07/02 11:14:27 PM | PATH=searchs/cifar10
07/02 11:14:27 PM | PLOT_PATH=searchs/cifar10/plots
07/02 11:14:27 PM | PRINT_FREQ=50
07/02 11:14:27 PM | SEED=2
07/02 11:14:27 PM | W_GRAD_CLIP=5.0
07/02 11:14:27 PM | W_LR=0.025
07/02 11:14:27 PM | W_LR_MIN=0.001
07/02 11:14:27 PM | W_MOMENTUM=0.9
07/02 11:14:27 PM | W_WEIGHT_DECAY=0.0003
07/02 11:14:27 PM | WORKERS=4
07/02 11:14:27 PM | 
07/02 11:14:27 PM | Logger is set - training start
07/02 11:17:55 PM | 
07/02 11:17:55 PM | Parameters:
07/02 11:17:55 PM | ALPHA_LR=0.0003
07/02 11:17:55 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 11:17:55 PM | BATCH_SIZE=32
07/02 11:17:55 PM | DATA_PATH=./data/
07/02 11:17:55 PM | DATASET=cifar10
07/02 11:17:55 PM | EPOCHS=1
07/02 11:17:55 PM | GPUS=[0]
07/02 11:17:55 PM | INIT_CHANNELS=16
07/02 11:17:55 PM | LAYERS=8
07/02 11:17:55 PM | NAME=cifar10
07/02 11:17:55 PM | PATH=searchs/cifar10
07/02 11:17:55 PM | PLOT_PATH=searchs/cifar10/plots
07/02 11:17:55 PM | PRINT_FREQ=50
07/02 11:17:55 PM | SEED=2
07/02 11:17:55 PM | W_GRAD_CLIP=5.0
07/02 11:17:55 PM | W_LR=0.025
07/02 11:17:55 PM | W_LR_MIN=0.001
07/02 11:17:55 PM | W_MOMENTUM=0.9
07/02 11:17:55 PM | W_WEIGHT_DECAY=0.0003
07/02 11:17:55 PM | WORKERS=4
07/02 11:17:55 PM | 
07/02 11:17:55 PM | Logger is set - training start
07/02 11:26:55 PM | 
07/02 11:26:55 PM | Parameters:
07/02 11:26:55 PM | ALPHA_LR=0.0003
07/02 11:26:55 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 11:26:55 PM | BATCH_SIZE=32
07/02 11:26:55 PM | DATA_PATH=./data/
07/02 11:26:55 PM | DATASET=cifar10
07/02 11:26:55 PM | EPOCHS=1
07/02 11:26:55 PM | GPUS=[0]
07/02 11:26:55 PM | INIT_CHANNELS=16
07/02 11:26:55 PM | LAYERS=8
07/02 11:26:55 PM | NAME=cifar10
07/02 11:26:55 PM | PATH=searchs/cifar10
07/02 11:26:55 PM | PLOT_PATH=searchs/cifar10/plots
07/02 11:26:55 PM | PRINT_FREQ=50
07/02 11:26:55 PM | SEED=2
07/02 11:26:55 PM | W_GRAD_CLIP=5.0
07/02 11:26:55 PM | W_LR=0.025
07/02 11:26:55 PM | W_LR_MIN=0.001
07/02 11:26:55 PM | W_MOMENTUM=0.9
07/02 11:26:55 PM | W_WEIGHT_DECAY=0.0003
07/02 11:26:55 PM | WORKERS=4
07/02 11:26:55 PM | 
07/02 11:26:55 PM | Logger is set - training start
07/02 11:31:38 PM | 
07/02 11:31:38 PM | Parameters:
07/02 11:31:38 PM | ALPHA_LR=0.0003
07/02 11:31:38 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 11:31:38 PM | BATCH_SIZE=32
07/02 11:31:38 PM | DATA_PATH=./data/
07/02 11:31:38 PM | DATASET=cifar10
07/02 11:31:38 PM | EPOCHS=1
07/02 11:31:38 PM | GPUS=[0]
07/02 11:31:38 PM | INIT_CHANNELS=16
07/02 11:31:38 PM | LAYERS=8
07/02 11:31:38 PM | NAME=cifar10
07/02 11:31:38 PM | PATH=searchs/cifar10
07/02 11:31:38 PM | PLOT_PATH=searchs/cifar10/plots
07/02 11:31:38 PM | PRINT_FREQ=50
07/02 11:31:38 PM | SEED=2
07/02 11:31:38 PM | W_GRAD_CLIP=5.0
07/02 11:31:38 PM | W_LR=0.025
07/02 11:31:38 PM | W_LR_MIN=0.001
07/02 11:31:38 PM | W_MOMENTUM=0.9
07/02 11:31:38 PM | W_WEIGHT_DECAY=0.0003
07/02 11:31:38 PM | WORKERS=4
07/02 11:31:38 PM | 
07/02 11:31:38 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/02 11:51:41 PM | 
07/02 11:51:41 PM | Parameters:
07/02 11:51:41 PM | ALPHA_LR=0.0003
07/02 11:51:41 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 11:51:41 PM | BATCH_SIZE=32
07/02 11:51:41 PM | DATA_PATH=./data/
07/02 11:51:41 PM | DATASET=cifar10
07/02 11:51:41 PM | EPOCHS=1
07/02 11:51:41 PM | GPUS=[0]
07/02 11:51:41 PM | INIT_CHANNELS=16
07/02 11:51:41 PM | LAYERS=8
07/02 11:51:41 PM | NAME=cifar10
07/02 11:51:41 PM | PATH=searchs/cifar10
07/02 11:51:41 PM | PLOT_PATH=searchs/cifar10/plots
07/02 11:51:41 PM | PRINT_FREQ=50
07/02 11:51:41 PM | SEED=2
07/02 11:51:41 PM | W_GRAD_CLIP=5.0
07/02 11:51:41 PM | W_LR=0.025
07/02 11:51:41 PM | W_LR_MIN=0.001
07/02 11:51:41 PM | W_MOMENTUM=0.9
07/02 11:51:41 PM | W_WEIGHT_DECAY=0.0003
07/02 11:51:41 PM | WORKERS=4
07/02 11:51:41 PM | 
07/02 11:51:41 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/02 11:55:07 PM | 
07/02 11:55:07 PM | Parameters:
07/02 11:55:07 PM | ALPHA_LR=0.0003
07/02 11:55:07 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 11:55:07 PM | BATCH_SIZE=32
07/02 11:55:07 PM | DATA_PATH=./data/
07/02 11:55:07 PM | DATASET=cifar10
07/02 11:55:07 PM | EPOCHS=1
07/02 11:55:07 PM | GPUS=[0]
07/02 11:55:07 PM | INIT_CHANNELS=16
07/02 11:55:07 PM | LAYERS=8
07/02 11:55:07 PM | NAME=cifar10
07/02 11:55:07 PM | PATH=searchs/cifar10
07/02 11:55:07 PM | PLOT_PATH=searchs/cifar10/plots
07/02 11:55:07 PM | PRINT_FREQ=50
07/02 11:55:07 PM | SEED=2
07/02 11:55:07 PM | W_GRAD_CLIP=5.0
07/02 11:55:07 PM | W_LR=0.025
07/02 11:55:07 PM | W_LR_MIN=0.001
07/02 11:55:07 PM | W_MOMENTUM=0.9
07/02 11:55:07 PM | W_WEIGHT_DECAY=0.0003
07/02 11:55:07 PM | WORKERS=4
07/02 11:55:07 PM | 
07/02 11:55:07 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/02 11:56:23 PM | 
07/02 11:56:23 PM | Parameters:
07/02 11:56:23 PM | ALPHA_LR=0.0003
07/02 11:56:23 PM | ALPHA_WEIGHT_DECAY=0.001
07/02 11:56:23 PM | BATCH_SIZE=32
07/02 11:56:23 PM | DATA_PATH=./data/
07/02 11:56:23 PM | DATASET=cifar10
07/02 11:56:23 PM | EPOCHS=1
07/02 11:56:23 PM | GPUS=[0]
07/02 11:56:23 PM | INIT_CHANNELS=16
07/02 11:56:23 PM | LAYERS=8
07/02 11:56:23 PM | NAME=cifar10
07/02 11:56:23 PM | PATH=searchs/cifar10
07/02 11:56:23 PM | PLOT_PATH=searchs/cifar10/plots
07/02 11:56:23 PM | PRINT_FREQ=50
07/02 11:56:23 PM | SEED=2
07/02 11:56:23 PM | W_GRAD_CLIP=5.0
07/02 11:56:23 PM | W_LR=0.025
07/02 11:56:23 PM | W_LR_MIN=0.001
07/02 11:56:23 PM | W_MOMENTUM=0.9
07/02 11:56:23 PM | W_WEIGHT_DECAY=0.0003
07/02 11:56:23 PM | WORKERS=4
07/02 11:56:23 PM | 
07/02 11:56:23 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/03 12:29:22 AM | 
07/03 12:29:22 AM | Parameters:
07/03 12:29:22 AM | ALPHA_LR=0.0003
07/03 12:29:22 AM | ALPHA_WEIGHT_DECAY=0.001
07/03 12:29:22 AM | BATCH_SIZE=32
07/03 12:29:22 AM | DATA_PATH=./data/
07/03 12:29:22 AM | DATASET=cifar10
07/03 12:29:22 AM | EPOCHS=1
07/03 12:29:22 AM | GPUS=[0]
07/03 12:29:22 AM | INIT_CHANNELS=16
07/03 12:29:22 AM | LAYERS=8
07/03 12:29:22 AM | NAME=cifar10
07/03 12:29:22 AM | PATH=searchs/cifar10
07/03 12:29:22 AM | PLOT_PATH=searchs/cifar10/plots
07/03 12:29:22 AM | PRINT_FREQ=50
07/03 12:29:22 AM | SEED=2
07/03 12:29:22 AM | W_GRAD_CLIP=5.0
07/03 12:29:22 AM | W_LR=0.025
07/03 12:29:22 AM | W_LR_MIN=0.001
07/03 12:29:22 AM | W_MOMENTUM=0.9
07/03 12:29:22 AM | W_WEIGHT_DECAY=0.0003
07/03 12:29:22 AM | WORKERS=4
07/03 12:29:22 AM | 
07/03 12:29:22 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/03 12:31:42 AM | 
07/03 12:31:42 AM | Parameters:
07/03 12:31:42 AM | ALPHA_LR=0.0003
07/03 12:31:42 AM | ALPHA_WEIGHT_DECAY=0.001
07/03 12:31:42 AM | BATCH_SIZE=32
07/03 12:31:42 AM | DATA_PATH=./data/
07/03 12:31:42 AM | DATASET=cifar10
07/03 12:31:42 AM | EPOCHS=1
07/03 12:31:42 AM | GPUS=[0]
07/03 12:31:42 AM | INIT_CHANNELS=16
07/03 12:31:42 AM | LAYERS=8
07/03 12:31:42 AM | NAME=cifar10
07/03 12:31:42 AM | PATH=searchs/cifar10
07/03 12:31:42 AM | PLOT_PATH=searchs/cifar10/plots
07/03 12:31:42 AM | PRINT_FREQ=50
07/03 12:31:42 AM | SEED=2
07/03 12:31:42 AM | W_GRAD_CLIP=5.0
07/03 12:31:42 AM | W_LR=0.025
07/03 12:31:42 AM | W_LR_MIN=0.001
07/03 12:31:42 AM | W_MOMENTUM=0.9
07/03 12:31:42 AM | W_WEIGHT_DECAY=0.0003
07/03 12:31:42 AM | WORKERS=4
07/03 12:31:42 AM | 
07/03 12:31:42 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/03 12:39:17 AM | 
07/03 12:39:17 AM | Parameters:
07/03 12:39:17 AM | ALPHA_LR=0.0003
07/03 12:39:17 AM | ALPHA_WEIGHT_DECAY=0.001
07/03 12:39:17 AM | BATCH_SIZE=32
07/03 12:39:17 AM | DATA_PATH=./data/
07/03 12:39:17 AM | DATASET=cifar10
07/03 12:39:17 AM | EPOCHS=1
07/03 12:39:17 AM | GPUS=[0]
07/03 12:39:17 AM | INIT_CHANNELS=16
07/03 12:39:17 AM | LAYERS=8
07/03 12:39:17 AM | NAME=cifar10
07/03 12:39:17 AM | PATH=searchs/cifar10
07/03 12:39:17 AM | PLOT_PATH=searchs/cifar10/plots
07/03 12:39:17 AM | PRINT_FREQ=50
07/03 12:39:17 AM | SEED=2
07/03 12:39:17 AM | W_GRAD_CLIP=5.0
07/03 12:39:17 AM | W_LR=0.025
07/03 12:39:17 AM | W_LR_MIN=0.001
07/03 12:39:17 AM | W_MOMENTUM=0.9
07/03 12:39:17 AM | W_WEIGHT_DECAY=0.0003
07/03 12:39:17 AM | WORKERS=4
07/03 12:39:17 AM | 
07/03 12:39:17 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/03 12:40:33 AM | 
07/03 12:40:33 AM | Parameters:
07/03 12:40:33 AM | ALPHA_LR=0.0003
07/03 12:40:33 AM | ALPHA_WEIGHT_DECAY=0.001
07/03 12:40:33 AM | BATCH_SIZE=32
07/03 12:40:33 AM | DATA_PATH=./data/
07/03 12:40:33 AM | DATASET=cifar10
07/03 12:40:33 AM | EPOCHS=1
07/03 12:40:33 AM | GPUS=[0]
07/03 12:40:33 AM | INIT_CHANNELS=16
07/03 12:40:33 AM | LAYERS=8
07/03 12:40:33 AM | NAME=cifar10
07/03 12:40:33 AM | PATH=searchs/cifar10
07/03 12:40:33 AM | PLOT_PATH=searchs/cifar10/plots
07/03 12:40:33 AM | PRINT_FREQ=50
07/03 12:40:33 AM | SEED=2
07/03 12:40:33 AM | W_GRAD_CLIP=5.0
07/03 12:40:33 AM | W_LR=0.025
07/03 12:40:33 AM | W_LR_MIN=0.001
07/03 12:40:33 AM | W_MOMENTUM=0.9
07/03 12:40:33 AM | W_WEIGHT_DECAY=0.0003
07/03 12:40:33 AM | WORKERS=4
07/03 12:40:33 AM | 
07/03 12:40:33 AM | Logger is set - training start
07/03 08:28:25 AM | 
07/03 08:28:25 AM | Parameters:
07/03 08:28:25 AM | ALPHA_LR=0.0003
07/03 08:28:25 AM | ALPHA_WEIGHT_DECAY=0.001
07/03 08:28:25 AM | BATCH_SIZE=32
07/03 08:28:25 AM | DATA_PATH=./data/
07/03 08:28:25 AM | DATASET=cifar10
07/03 08:28:25 AM | EPOCHS=1
07/03 08:28:25 AM | GPUS=[0]
07/03 08:28:25 AM | INIT_CHANNELS=16
07/03 08:28:25 AM | LAYERS=8
07/03 08:28:25 AM | NAME=cifar10
07/03 08:28:25 AM | PATH=searchs/cifar10
07/03 08:28:25 AM | PLOT_PATH=searchs/cifar10/plots
07/03 08:28:25 AM | PRINT_FREQ=50
07/03 08:28:25 AM | SEED=2
07/03 08:28:25 AM | W_GRAD_CLIP=5.0
07/03 08:28:25 AM | W_LR=0.025
07/03 08:28:25 AM | W_LR_MIN=0.001
07/03 08:28:25 AM | W_MOMENTUM=0.9
07/03 08:28:25 AM | W_WEIGHT_DECAY=0.0003
07/03 08:28:25 AM | WORKERS=4
07/03 08:28:25 AM | 
07/03 08:28:25 AM | Logger is set - training start
07/03 08:31:29 AM | 
07/03 08:31:29 AM | Parameters:
07/03 08:31:29 AM | ALPHA_LR=0.0003
07/03 08:31:29 AM | ALPHA_WEIGHT_DECAY=0.001
07/03 08:31:29 AM | BATCH_SIZE=32
07/03 08:31:29 AM | DATA_PATH=./data/
07/03 08:31:29 AM | DATASET=cifar10
07/03 08:31:29 AM | EPOCHS=1
07/03 08:31:29 AM | GPUS=[0]
07/03 08:31:29 AM | INIT_CHANNELS=16
07/03 08:31:29 AM | LAYERS=8
07/03 08:31:29 AM | NAME=cifar10
07/03 08:31:29 AM | PATH=searchs/cifar10
07/03 08:31:29 AM | PLOT_PATH=searchs/cifar10/plots
07/03 08:31:29 AM | PRINT_FREQ=50
07/03 08:31:29 AM | SEED=2
07/03 08:31:29 AM | W_GRAD_CLIP=5.0
07/03 08:31:29 AM | W_LR=0.025
07/03 08:31:29 AM | W_LR_MIN=0.001
07/03 08:31:29 AM | W_MOMENTUM=0.9
07/03 08:31:29 AM | W_WEIGHT_DECAY=0.0003
07/03 08:31:29 AM | WORKERS=4
07/03 08:31:29 AM | 
07/03 08:31:29 AM | Logger is set - training start
07/03 08:32:19 AM | 
07/03 08:32:19 AM | Parameters:
07/03 08:32:19 AM | ALPHA_LR=0.0003
07/03 08:32:19 AM | ALPHA_WEIGHT_DECAY=0.001
07/03 08:32:19 AM | BATCH_SIZE=32
07/03 08:32:19 AM | DATA_PATH=./data/
07/03 08:32:19 AM | DATASET=cifar10
07/03 08:32:19 AM | EPOCHS=1
07/03 08:32:19 AM | GPUS=[0]
07/03 08:32:19 AM | INIT_CHANNELS=16
07/03 08:32:19 AM | LAYERS=8
07/03 08:32:19 AM | NAME=cifar10
07/03 08:32:19 AM | PATH=searchs/cifar10
07/03 08:32:19 AM | PLOT_PATH=searchs/cifar10/plots
07/03 08:32:19 AM | PRINT_FREQ=50
07/03 08:32:19 AM | SEED=2
07/03 08:32:19 AM | W_GRAD_CLIP=5.0
07/03 08:32:19 AM | W_LR=0.025
07/03 08:32:19 AM | W_LR_MIN=0.001
07/03 08:32:19 AM | W_MOMENTUM=0.9
07/03 08:32:19 AM | W_WEIGHT_DECAY=0.0003
07/03 08:32:19 AM | WORKERS=4
07/03 08:32:19 AM | 
07/03 08:32:19 AM | Logger is set - training start
07/03 08:37:12 AM | 
07/03 08:37:12 AM | Parameters:
07/03 08:37:12 AM | ALPHA_LR=0.0003
07/03 08:37:12 AM | ALPHA_WEIGHT_DECAY=0.001
07/03 08:37:12 AM | BATCH_SIZE=32
07/03 08:37:12 AM | DATA_PATH=./data/
07/03 08:37:12 AM | DATASET=cifar10
07/03 08:37:12 AM | EPOCHS=1
07/03 08:37:12 AM | GPUS=[0]
07/03 08:37:12 AM | INIT_CHANNELS=16
07/03 08:37:12 AM | LAYERS=8
07/03 08:37:12 AM | NAME=cifar10
07/03 08:37:12 AM | PATH=searchs/cifar10
07/03 08:37:12 AM | PLOT_PATH=searchs/cifar10/plots
07/03 08:37:12 AM | PRINT_FREQ=50
07/03 08:37:12 AM | SEED=2
07/03 08:37:12 AM | W_GRAD_CLIP=5.0
07/03 08:37:12 AM | W_LR=0.025
07/03 08:37:12 AM | W_LR_MIN=0.001
07/03 08:37:12 AM | W_MOMENTUM=0.9
07/03 08:37:12 AM | W_WEIGHT_DECAY=0.0003
07/03 08:37:12 AM | WORKERS=4
07/03 08:37:12 AM | 
07/03 08:37:12 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
